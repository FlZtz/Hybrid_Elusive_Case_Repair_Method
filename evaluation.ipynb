{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e601f57622a420"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from model import Transformer\n",
    "from config import get_config, get_weights_file_path, reset_log\n",
    "from train import get_model, get_ds, greedy_decode, create_log\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:57:16.888232300Z",
     "start_time": "2024-02-07T11:57:06.709702800Z"
    }
   },
   "id": "9275dee67bd090df",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:57:17.793386300Z",
     "start_time": "2024-02-07T11:57:17.761775100Z"
    }
   },
   "id": "f86b11c6dfa4ec8c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_next_batch() -> Tuple[dict, list, list]:\n",
    "    \"\"\"\n",
    "    Load a sample batch from the validation set and return relevant tensors and tokens.\n",
    "    \n",
    "    :return: A dictionary containing encoder and decoder input tensors; List of tokens for the encoder input; \n",
    "    List of tokens for the decoder input.\n",
    "    \"\"\"\n",
    "    # Load a sample batch from the validation set\n",
    "    batch = next(iter(val_dataloader))\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "    decoder_input = batch[\"decoder_input\"].to(device)\n",
    "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "    encoder_input_tokens = [vocab_src.id_to_token(idx) for idx in encoder_input[0].cpu().numpy()]\n",
    "    decoder_input_tokens = [vocab_tgt.id_to_token(idx) for idx in decoder_input[0].cpu().numpy()]\n",
    "\n",
    "    # check that the batch size is 1\n",
    "    assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "    model_out = greedy_decode(model, encoder_input, encoder_mask, vocab_src, vocab_tgt, config['seq_len'], device)\n",
    "    \n",
    "    return batch, encoder_input_tokens, decoder_input_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:57:18.542766100Z",
     "start_time": "2024-02-07T11:57:18.514584800Z"
    }
   },
   "id": "2d32db476dfb6017",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def mtx2df(m: np.ndarray, max_row: int, max_col: int, row_tokens: list, col_tokens: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a matrix to a pandas DataFrame.\n",
    "    \n",
    "    :param m: Input matrix.\n",
    "    :param max_row: Maximum number of rows to include in the DataFrame.\n",
    "    :param max_col: Maximum number of columns to include in the DataFrame.\n",
    "    :param row_tokens: List of row tokens.\n",
    "    :param col_tokens: List of column tokens.\n",
    "    :return: DataFrame representation of the input matrix.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),\n",
    "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
    "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
    "            )\n",
    "            for r in range(m.shape[0])\n",
    "            for c in range(m.shape[1])\n",
    "            if r < max_row and c < max_col\n",
    "        ],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_attn_map(attn_type: str, layer: int, head: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get attention maps based on attention type, layer, and head.\n",
    "    \n",
    "    :param attn_type: Type of attention ('encoder', 'decoder', or 'encoder-decoder')\n",
    "    :param layer: Layer index.\n",
    "    :param head: Head index.\n",
    "    :return: Attention map tensor.\n",
    "    \"\"\"\n",
    "    if attn_type == \"encoder\":\n",
    "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
    "    return attn[0, head].data\n",
    "\n",
    "\n",
    "def attn_map(attn_type: str, layer: int, head: int, row_tokens: list, col_tokens: list, max_sentence_len: int) -> alt.Chart:\n",
    "    \"\"\"\n",
    "    Create an attention map for a specific type, layer, and head.\n",
    "    \n",
    "    :param attn_type: Type of attention ('encoder', 'decoder', or 'encoder-decoder').\n",
    "    :param layer: Layer index.\n",
    "    :param head: Head index.\n",
    "    :param row_tokens: List of row tokens.\n",
    "    :param col_tokens: List of column tokens.\n",
    "    :param max_sentence_len: Maximum length of the sentence.\n",
    "    :return: Altair chart object representing the attention map.\n",
    "    \"\"\"\n",
    "    df = mtx2df(\n",
    "        get_attn_map(attn_type, layer, head),\n",
    "        max_sentence_len,\n",
    "        max_sentence_len,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data=df)\n",
    "        .mark_rect()\n",
    "        .encode(\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_all_attention_maps(attn_type: str, layers: list, heads: list, row_tokens: list, col_tokens: list, max_sentence_len: int) -> alt.ConcatChart:\n",
    "    \"\"\"\n",
    "    Get all attention maps for specified types, layers, and heads.\n",
    "    \n",
    "    :param attn_type: Type of attention ('encoder', 'decoder', or 'encoder-decoder').\n",
    "    :param layers: List of layer indices.\n",
    "    :param heads: List of head indices.\n",
    "    :param row_tokens: List of row tokens.\n",
    "    :param col_tokens: List of column tokens.\n",
    "    :param max_sentence_len: Maximum length of the sentence.\n",
    "    :return: Concatenated Altair chart object representing all attention maps\n",
    "    \"\"\"\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "        charts.append(alt.hconcat(*rowCharts))\n",
    "    return alt.vconcat(*charts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:57:20.091919200Z",
     "start_time": "2024-02-07T11:57:20.041656700Z"
    }
   },
   "id": "db1f5230c3231ac2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def matching_proportion(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Calculate and print the proportion of matching case IDs.\n",
    "    \n",
    "    :param df: DataFrame containing case IDs.\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is not empty before calculating the proportion\n",
    "    if not df.empty:\n",
    "        # Count the number of rows where 'Determined Case ID' and 'Actual Case ID' match\n",
    "        matching_rows = df[df['Determined Case ID'] == df['Actual Case ID']]\n",
    "\n",
    "        # Calculate the proportion of matching rows\n",
    "        proportion_matching = len(matching_rows) / len(df)\n",
    "\n",
    "        print(f\"{len(matching_rows)} of {len(df)} case ID{'s' if len(df) != 1 else ''} \"\n",
    "              f\"{'was' if len(matching_rows) == 1 else 'were'} determined correctly. \"\n",
    "              f\"This corresponds to an accuracy of {proportion_matching:.2%}.\")\n",
    "    else:\n",
    "        print(\"DataFrame is empty. Cannot calculate accuracy.\")\n",
    "\n",
    "    \n",
    "def completely_correct_cases(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Calculate and print the proportion of completely correct case IDs.\n",
    "    \n",
    "    :param df: DataFrame containing case IDs.\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is not empty before calculating the proportion\n",
    "    if not df.empty:\n",
    "        cases = df['Actual Case ID'].unique()\n",
    "\n",
    "        # Initialize an array to store values that meet both conditions\n",
    "        matching_values = []\n",
    "\n",
    "        # Iterate through unique values\n",
    "        for case_id in cases:\n",
    "            # Filter rows for the current 'Actual Case ID'\n",
    "            subset_actual = df[df['Actual Case ID'] == case_id]\n",
    "\n",
    "            # Check if 'Actual Case ID' equals 'Determined Case ID' for all rows\n",
    "            condition_1 = all(subset_actual['Actual Case ID'] == subset_actual['Determined Case ID'])\n",
    "\n",
    "            # Filter rows for the current 'Determined Case ID'\n",
    "            subset_determined = df[df['Determined Case ID'] == case_id]\n",
    "\n",
    "            # Check if 'Determined Case ID' equals 'Actual Case ID' for all rows\n",
    "            condition_2 = all(subset_determined['Determined Case ID'] == subset_determined['Actual Case ID'])\n",
    "\n",
    "            # If both conditions are true, add the value to the array\n",
    "            if condition_1 and condition_2:\n",
    "                matching_values.append(case_id)\n",
    "\n",
    "        # Print the number of completely correct cases\n",
    "        print(f\"{len(matching_values)} of {len(cases)} case{'s' if len(cases) != 1 else ''} \"\n",
    "              f\"{'were' if len(matching_values) != 1 else 'was'} determined completely correctly.\", end=' ')\n",
    "\n",
    "        # Print the list of completely correct cases (using \"and\" before the last value)\n",
    "        if matching_values:\n",
    "            if len(matching_values) == 1:\n",
    "                print(f\"This is {matching_values[0]}.\", end=' ')\n",
    "            else:\n",
    "                print(f\"These are {', '.join(map(str, matching_values[:-1]))} and {matching_values[-1]}.\", end=' ')\n",
    "\n",
    "        # Calculate the proportion of completely correct cases\n",
    "        proportion_complete_matching = len(matching_values) / len(cases)\n",
    "\n",
    "        print(f\"This corresponds to a case accuracy of {proportion_complete_matching:.2%}.\")\n",
    "    else:\n",
    "        print(\"DataFrame is empty. Cannot calculate case accuracy.\")\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate model metrics based on the determined log.\n",
    "    \n",
    "    :param df: DataFrame containing determined log.\n",
    "    \"\"\"\n",
    "    matching_proportion(df)\n",
    "    completely_correct_cases(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:57:20.761467300Z",
     "start_time": "2024-02-07T11:57:20.731049500Z"
    }
   },
   "id": "333713122ca55a11",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d434ea7486cf04c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b849b5d90bae3b43"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "parsing log, completed traces ::   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5e677cd2e3d407d9016f23a287d7174"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following columns were automatically matched:\n",
      "'case:concept:name' for 'Case ID';\n",
      "'concept:name' for 'Activity';\n",
      "'time:timestamp' for 'Timestamp'.\n",
      "Max length of source sentence: 10\n",
      "Max length of target sentence: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_log()  # Reset log file\n",
    "config = get_config()  # Get configuration parameters\n",
    "train_dataloader, val_dataloader, vocab_src, vocab_tgt = get_ds(config)  # Get training and validation data\n",
    "model = get_model(config, vocab_src.get_vocab_size(), vocab_tgt.get_vocab_size()).to(device)  # Get the model\n",
    "\n",
    "# Load pretrained weights for the model\n",
    "model_filename = get_weights_file_path(config, f\"19\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:57:48.795925400Z",
     "start_time": "2024-02-07T11:57:22.032243900Z"
    }
   },
   "id": "f4b4e0d0f8e81bae",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load next batch from validation set and print source and target texts\n",
    "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "print(f'Source: {batch[\"src_text\"][0]}')\n",
    "print(f'Target: {batch[\"tgt_text\"][0]}')\n",
    "\n",
    "sentence_len = 10  # Length of the sentence\n",
    "layers = [0, 1, 2]  # Layers for attention maps\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]  # Heads for attention maps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bd7b54ec82b87cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display attention maps for encoder self-attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15bc638b5cd31c46",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display attention maps for decoder self-attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "920c0b483535619f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display attention maps for cross-attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92a353a688a23600",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log = create_log(config)  # Create log based on configuration\n",
    "\n",
    "# Display the first 20 rows of the log\n",
    "print(log.head(20))\n",
    "\n",
    "# Check if there are more than 20 rows in the log and print if so\n",
    "remaining_rows = len(log) - 20\n",
    "if remaining_rows > 0:\n",
    "    print(f\"\\n... (+ {remaining_rows} more rows)\")\n",
    "\n",
    "print('-' * 80)\n",
    "\n",
    "# Evaluate model metrics based on the determined log\n",
    "evaluate_model_metrics(log)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95157de5a48bc4a9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Review Example Large"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36e0c15af21a6d6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reset_log()  # Reset log file\n",
    "config = get_config()  # Get configuration parameters\n",
    "train_dataloader, val_dataloader, vocab_src, vocab_tgt = get_ds(config)  # Get training and validation data\n",
    "model = get_model(config, vocab_src.get_vocab_size(), vocab_tgt.get_vocab_size()).to(device)  # Get the model\n",
    "\n",
    "# Load pretrained weights for the model\n",
    "model_filename = get_weights_file_path(config, f\"19\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77504f8868f020f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load next batch from validation set and print source and target texts\n",
    "batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "print(f'Source: {batch[\"src_text\"][0]}')\n",
    "print(f'Target: {batch[\"tgt_text\"][0]}')\n",
    "\n",
    "sentence_len = 10  # Length of the sentence\n",
    "layers = [0, 1, 2]  # Layers for attention maps\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]  # Heads for attention maps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e15496b4b38f7509"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display attention maps for encoder self-attention\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, min(20, sentence_len))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f3c93e20101dc8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display attention maps for decoder self-attention\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e233ecdcf51dc9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display attention maps for cross-attention\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_tokens, decoder_input_tokens, min(20, sentence_len))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97494f376737cb24"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log = create_log(config)  # Create log based on configuration\n",
    "\n",
    "# Display the first 20 rows of the log\n",
    "print(log.head(20))\n",
    "\n",
    "# Check if there are more than 20 rows in the log and print if so\n",
    "remaining_rows = len(log) - 20\n",
    "if remaining_rows > 0:\n",
    "    print(f\"\\n... (+ {remaining_rows} more rows)\")\n",
    "\n",
    "print('-' * 80)\n",
    "\n",
    "# Evaluate model metrics based on the determined log\n",
    "evaluate_model_metrics(log)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f653c3c292c3d6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
