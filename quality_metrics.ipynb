{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation of Output Quality",
   "id": "373d3274ab50f5b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:03.798256Z",
     "start_time": "2024-04-27T16:55:59.036115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "from config import extract_log_name, get_file_path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "7589758cab53053d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:03.814018Z",
     "start_time": "2024-04-27T16:56:03.799295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_elusive_equivalents(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add the elusive metrics to the metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the metrics.\n",
    "    :return: DataFrame containing the metrics with the elusive metrics added.\n",
    "    \"\"\"\n",
    "    start_completeness = metrics.loc[metrics['Iteration'] == 0, 'Completeness'].values[0]\n",
    "    \n",
    "    if start_completeness > 0:\n",
    "        for metric in [\"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\"]:\n",
    "            start_metric = metrics.loc[metrics['Iteration'] == 0, metric].values[0]\n",
    "            column_index = metrics.columns.get_loc(metric) + 1\n",
    "            metrics.insert(column_index, f\"Elus. {metric}\", None)\n",
    "\n",
    "            for i in metrics.index:\n",
    "                iteration_metric = metrics.loc[i, metric]\n",
    "                difference = iteration_metric - start_metric\n",
    "                added_metric = (difference / (100 - start_metric)) * 100\n",
    "                metrics.loc[i, f\"Elus. {metric}\"] = added_metric\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "c6aa4d0c7936acb4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:03.862355Z",
     "start_time": "2024-04-27T16:56:03.814636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_completely_correct_cases(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of completely correct Case ID values.\n",
    "\n",
    "    A completely correct Case ID value is one where the 'Case ID' matches the 'Determined Case ID' for all rows where\n",
    "    it appears, and vice versa.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = all(subset_correct['Case ID'] == subset_predicted[column])\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        subset_determined = predicted_df[predicted_df[column] == case_id]\n",
    "        subset_cases = correct_df.loc[subset_determined.index]\n",
    "\n",
    "        condition_2 = all(subset_determined[column] == subset_cases['Case ID'])\n",
    "\n",
    "        if condition_2:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    proportion_completely_correct = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_completely_correct\n",
    "\n",
    "\n",
    "def calculate_correct_case_different_naming(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str,\n",
    "                                            calculation: bool = True) -> Union[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually completely correct Case ID values.\n",
    "\n",
    "    A factually completely correct Case ID value is one where the Determined Case ID is different from the Case ID, yet\n",
    "    uniquely maps back to the same Case ID value.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :param calculation: If True, calculate the proportion of factually completely correct Case ID values.\n",
    "                        If False, return the list of factually completely correct Case ID values.\n",
    "    :return: Proportion of factually completely correct Case ID values if calculation is True,\n",
    "             otherwise return the DataFrame of factually completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0 if calculation else pd.DataFrame()\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = len(subset_predicted[column].unique()) == 1\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        unique_value = subset_predicted[column].iloc[0]\n",
    "\n",
    "        condition_2 = unique_value != case_id\n",
    "\n",
    "        if not condition_2:\n",
    "            continue\n",
    "\n",
    "        condition_3 = not predicted_df[(predicted_df[column] == unique_value) & \n",
    "                                       (correct_df['Case ID'] != case_id)].any().any()\n",
    "\n",
    "        if condition_3:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    if not calculation:\n",
    "        matching_rows = correct_df[correct_df['Case ID'].isin(correct_cases)]\n",
    "        return matching_rows\n",
    "\n",
    "    proportion_correct_different_naming = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_correct_different_naming\n",
    "\n",
    "\n",
    "def calculate_factual_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of factually matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = calculate_correct_case_different_naming(predicted_df, correct_df, column, False)\n",
    "    proportion_factual_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_factual_matching\n",
    "\n",
    "\n",
    "def calculate_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = predicted_df[predicted_df[column] == correct_df['Case ID']]\n",
    "    proportion_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_matching\n",
    "\n",
    "\n",
    "def evaluate_accuracy(predicted_df: pd.DataFrame, complete_df: pd.DataFrame, \n",
    "                      column: str = 'Determined Case ID') -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the repaired log.\n",
    "\n",
    "    :param predicted_df: DataFrame containing the determined log.\n",
    "    :param complete_df: DataFrame containing complete log.\n",
    "    :param column: Column to evaluate accuracy. Default is 'Determined Case ID'.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    matching = calculate_matching_proportion(predicted_df, complete_df, column)\n",
    "    factual_matching = calculate_factual_matching_proportion(predicted_df, complete_df, column)\n",
    "    correct_proportion = calculate_completely_correct_cases(predicted_df, complete_df, column)\n",
    "    factual_correct_proportion = calculate_correct_case_different_naming(predicted_df, complete_df, column)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": matching,\n",
    "        \"Factual Accuracy\": factual_matching,\n",
    "        \"Overall Accuracy\": matching + factual_matching,\n",
    "        \"Real Case Accuracy\": correct_proportion,\n",
    "        \"Factual Case Accuracy\": factual_correct_proportion,\n",
    "        \"Overall Case Accuracy\": correct_proportion + factual_correct_proportion\n",
    "    }\n"
   ],
   "id": "150b7690c957beff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:03.925379Z",
     "start_time": "2024-04-27T16:56:03.880105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_completeness(df: pd.DataFrame, column: str = 'Determined Case ID') -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the completeness of the log.\n",
    "\n",
    "    :param df: DataFrame containing the log.\n",
    "    :param column: Column to evaluate completeness. Default is 'Determined Case ID'.\n",
    "    :return: Proportion of missing values in the 'Case ID' column.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return float('-inf')\n",
    "\n",
    "    percentage_not_na = (1 - df[column].isna().sum() / len(df[column])) * 100\n",
    "    return percentage_not_na\n"
   ],
   "id": "1d1fbcc9e37b07b9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:03.988285Z",
     "start_time": "2024-04-27T16:56:03.925379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_directly_following_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of cases containing solely correct directly following activities.\n",
    "\n",
    "    Solely correct directly following activities means that, for each case, it verifies whether the directly following \n",
    "    activities occur in the correct order and have the same non-zero number of predecessors and successors as defined \n",
    "    in the configuration.\n",
    "    \n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct directly following activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_directly_following = 0\n",
    "    directly_following = configuration['expert_input_values']['Directly Following']\n",
    "    always_directly_following = [pair for pair, occurrence in zip(\n",
    "        directly_following['values'], directly_following['occurrences']) if occurrence == 'always']\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            is_consistent_case = True\n",
    "            \n",
    "            for predecessor, successor in always_directly_following:\n",
    "                if not is_consistent_case:\n",
    "                    break\n",
    "                positions_predecessor = group[group['Activity'] == predecessor].index.tolist()\n",
    "                positions_successor = group[group['Activity'] == successor].index.tolist()\n",
    "\n",
    "                if not positions_predecessor or not positions_successor:\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "                \n",
    "                if len(positions_predecessor) != len(positions_successor):\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "                \n",
    "                first_predecessor, last_predecessor = positions_predecessor[0], positions_predecessor[-1]\n",
    "                first_successor, last_successor = positions_successor[0], positions_successor[-1]\n",
    "                \n",
    "                if first_predecessor > first_successor or last_predecessor > last_successor:\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "\n",
    "            if is_consistent_case:\n",
    "                num_correct_directly_following += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_directly_following / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def calculate_end_activity_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of correct end activities.\n",
    "\n",
    "    A correct end activity is one that is present in the expert input values for the end activity.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct end activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_end = 0\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            end_activity = group['Activity'].iloc[-1]\n",
    "            if end_activity in configuration['expert_input_values']['End Activity']['values']:\n",
    "                num_correct_end += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_end / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def calculate_start_activity_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of correct start activities.\n",
    "\n",
    "    A correct start activity is one that is present in the expert input values for the start activity.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct start activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_start = 0\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            start_activity = group['Activity'].iloc[0]\n",
    "            if start_activity in configuration['expert_input_values']['Start Activity']['values']:\n",
    "                num_correct_start += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_start / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def evaluate_consistency(df: pd.DataFrame, configuration: dict, column: str = 'Determined Case ID') -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the consistency of the log.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency. Default is 'Determined Case ID'.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    if not configuration['expert_input_attributes'] or column not in df.columns:\n",
    "        return {}\n",
    "    \n",
    "    start_activity_consistency, end_activity_consistency, directly_following_consistency = None, None, None\n",
    "\n",
    "    for attribute in configuration['expert_input_attributes']:\n",
    "        if attribute == 'Start Activity':\n",
    "            start_activity_consistency = calculate_start_activity_consistency(df, configuration, column)\n",
    "        elif attribute == 'End Activity':\n",
    "            end_activity_consistency = calculate_end_activity_consistency(df, configuration, column)\n",
    "        elif attribute == 'Directly Following':\n",
    "            directly_following_consistency = calculate_directly_following_consistency(df, configuration, column)\n",
    "\n",
    "    return {\n",
    "        \"St. Ac. Consistency\": start_activity_consistency,\n",
    "        \"End Ac. Consistency\": end_activity_consistency,\n",
    "        \"Di. Fo. Consistency\": directly_following_consistency\n",
    "    }\n"
   ],
   "id": "6de1ecf6abac8cab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:04.019959Z",
     "start_time": "2024-04-27T16:56:03.988285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_repaired_logs(folder_path: str, log_name: str, complete_log: pd.DataFrame, configuration: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the repaired logs.\n",
    "\n",
    "    :param folder_path: Path to the folder containing the repaired logs.\n",
    "    :param log_name: Name of the log.\n",
    "    :param complete_log: DataFrame containing the complete log.\n",
    "    :param configuration: Dictionary containing the expert input used for training.\n",
    "    :return: DataFrame containing the quality metrics.\n",
    "    \"\"\"\n",
    "    quality_metrics = pd.DataFrame(columns=[\n",
    "        \"Iteration\", \"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\", \"Real Case Accuracy\",\n",
    "        \"Factual Case Accuracy\", \"Overall Case Accuracy\", \"St. Ac. Consistency\", \"End Ac. Consistency\", \n",
    "        \"Di. Fo. Consistency\"\n",
    "    ])\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        num_iterations = len([f for f in os.listdir(folder_path) if f.endswith('.csv')\n",
    "                              and f.startswith(f\"determined_{log_name}_iteration_\")])\n",
    "\n",
    "        if num_iterations:\n",
    "            for i in range(1, num_iterations + 1):\n",
    "                log_file = os.path.join(folder_path, f\"determined_{log_name}_iteration_{i}.csv\")\n",
    "\n",
    "                if os.path.exists(log_file):\n",
    "                    predicted_log = pd.read_csv(log_file)\n",
    "\n",
    "                    if i == 1:\n",
    "                        completeness = evaluate_completeness(predicted_log, 'Original Case ID')\n",
    "                        iteration_metrics = {\"Iteration\": 0, \"Completeness\": completeness}\n",
    "                        accuracy = evaluate_accuracy(predicted_log, complete_log, 'Original Case ID')\n",
    "                        iteration_metrics.update(accuracy)\n",
    "                        consistency = evaluate_consistency(predicted_log, configuration, 'Original Case ID')\n",
    "                        iteration_metrics.update(consistency)\n",
    "                        quality_metrics = pd.DataFrame([iteration_metrics])\n",
    "                    completeness = evaluate_completeness(predicted_log)\n",
    "                    iteration_metrics = {\"Iteration\": i, \"Completeness\": completeness}\n",
    "                    accuracy = evaluate_accuracy(predicted_log, complete_log)\n",
    "                    iteration_metrics.update(accuracy)\n",
    "                    consistency = evaluate_consistency(predicted_log, configuration)\n",
    "                    iteration_metrics.update(consistency)\n",
    "                    quality_metrics = pd.concat([quality_metrics, pd.DataFrame([iteration_metrics])], ignore_index=True)\n",
    "      \n",
    "    possible_expert_attributes = ['Start Activity', 'End Activity', 'Directly Following']\n",
    "    included_attributes = [attr for attr in possible_expert_attributes if attr in configuration['expert_input_attributes']]\n",
    "    missing_attributes = [attr for attr in possible_expert_attributes if attr not in configuration['expert_input_attributes']]\n",
    "    \n",
    "    if included_attributes:\n",
    "        if ('Directly Following' in included_attributes and \n",
    "                'always' not in configuration['expert_input_values']['Directly Following']['occurrences']):\n",
    "            quality_metrics.drop(columns=\"Di. Fo. Consistency\", inplace=True)\n",
    "    \n",
    "    if missing_attributes:\n",
    "        if 'Start Activity' in missing_attributes:\n",
    "            quality_metrics.drop(columns=\"St. Ac. Consistency\", inplace=True)\n",
    "        if 'End Activity' in missing_attributes:\n",
    "            quality_metrics.drop(columns=\"End Ac. Consistency\", inplace=True)\n",
    "        if 'Directly Following' in missing_attributes:\n",
    "            quality_metrics.drop(columns=\"Di. Fo. Consistency\", inplace=True)\n",
    "    \n",
    "\n",
    "    return quality_metrics\n"
   ],
   "id": "15b37fa02ad6db9d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:04.036063Z",
     "start_time": "2024-04-27T16:56:04.019959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_metrics(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the quality metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the quality metrics.\n",
    "    :return: Formatted DataFrame.\n",
    "    \"\"\"\n",
    "    if metrics.empty:\n",
    "        return metrics\n",
    "\n",
    "    cols_to_format = metrics.columns.drop('Iteration')\n",
    "    metrics[cols_to_format] = metrics[cols_to_format].applymap(lambda x: '{:.2f}%'.format(x) if pd.notnull(x) else \"\")\n",
    "    metrics['Iteration'] = metrics['Iteration'].astype(int)\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "d8d8f1a9cedfcf2d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:04.068150Z",
     "start_time": "2024-04-27T16:56:04.039167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_input() -> Tuple[pd.DataFrame, str, str, dict]:\n",
    "    \"\"\"\n",
    "    Prompt the user to input the path to the folder containing the repaired logs as well as the complete log with \n",
    "    corresponding name, and retrieve the model configuration if available.\n",
    "\n",
    "    :return: A tuple containing the complete log DataFrame, the name of the complete log file, the path to the folder \n",
    "     containing the repaired logs after each iteration, and a dictionary representing the model configuration if \n",
    "     available, otherwise an empty dictionary.\n",
    "    \"\"\"\n",
    "    complete_log_path = get_file_path(\"preprocessed complete log\")\n",
    "\n",
    "    if os.path.exists(complete_log_path):\n",
    "        complete_log = pd.read_csv(complete_log_path)\n",
    "        print(\"CSV file successfully read.\")\n",
    "        log_name = extract_log_name(complete_log_path)\n",
    "\n",
    "        if \"DISPLAY\" in os.environ:\n",
    "            root = tk.Tk()\n",
    "            root.withdraw()\n",
    "\n",
    "            folder_path = filedialog.askdirectory(\n",
    "                title=\"Select the folder that contains the repaired logs after each iteration\")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "        else:\n",
    "            folder_path = input(\"Enter the path to the folder that contains the repaired logs after each iteration: \")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "\n",
    "            folder_path = folder_path.strip('\"')\n",
    "\n",
    "        print(\"Folder path successfully read.\")\n",
    "        \n",
    "        configuration_path = get_file_path(\"model configuration\")\n",
    "        \n",
    "        if os.path.exists(configuration_path):\n",
    "            with open(configuration_path, 'rb') as file:\n",
    "                model_configuration = pickle.load(file)\n",
    "                print(\"Model configuration file successfully read.\")\n",
    "            \n",
    "            model_configuration = {key: value.to_dict() if isinstance(value, pd.DataFrame) else value for key, value in\n",
    "                                   model_configuration.items()}\n",
    "\n",
    "            return complete_log, log_name, folder_path, model_configuration\n",
    "        \n",
    "        return complete_log, log_name, folder_path, {}\n",
    "\n",
    "    return pd.DataFrame(), \"\", \"\", {}\n"
   ],
   "id": "f635854ae114b1f4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:04.084008Z",
     "start_time": "2024-04-27T16:56:04.070357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_metrics(metrics: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print the evaluation metrics in a tabular format.\n",
    "    \n",
    "    :param metrics: DataFrame containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    num_columns = len(metrics.columns) - 1\n",
    "    num_tables = (num_columns + 3) // 4\n",
    "    \n",
    "    for i in range(num_tables):\n",
    "        start_idx = 1 + 4 * i\n",
    "        end_idx = min(start_idx + 4, num_columns + 1)\n",
    "        indices = [0] + list(range(start_idx, end_idx))\n",
    "        metrics_table = metrics.iloc[:, indices]\n",
    "        print(tabulate(metrics_table, headers='keys', tablefmt='grid', showindex=False))\n"
   ],
   "id": "57f4364067a174a4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:04.099643Z",
     "start_time": "2024-04-27T16:56:04.084008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_metrics(metrics: pd.DataFrame, log_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to a CSV file.\n",
    "    \n",
    "    :param metrics: DataFrame containing evaluation metrics.\n",
    "    :param log_name: Name of the log file.\n",
    "    \"\"\"\n",
    "    folder_name = f\"evaluation/{log_name}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    current_time = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    file_name = f'metrics_{current_time}.csv'\n",
    "    metrics.to_csv(os.path.join(folder_name, file_name), index=False)\n"
   ],
   "id": "2b88746e93f1974d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T16:56:19.732253Z",
     "start_time": "2024-04-27T16:56:06.123714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "complete_log, log_name, folder_path, model_configuration = get_input()\n",
    "\n",
    "if folder_path:\n",
    "    metrics = evaluate_repaired_logs(folder_path, log_name, complete_log, model_configuration)\n",
    "    metrics = add_elusive_equivalents(metrics)\n",
    "    metrics = format_metrics(metrics)\n",
    "    save_metrics(metrics, log_name)\n",
    "    print_metrics(metrics)\n"
   ],
   "id": "8364b4c7b44adda0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully read.\n",
      "Folder path successfully read.\n",
      "Model configuration file successfully read.\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         | 0.00%                | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 92.86%         | 62.50%               | 83.33%     | 12.50%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           2 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           3 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           4 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 83.33%             | 12.50%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           2 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           3 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           4 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|   Iteration | Real Case Accuracy   | Factual Case Accuracy   | Overall Case Accuracy   | St. Ac. Consistency   |\n",
      "+=============+======================+=========================+=========================+=======================+\n",
      "|           0 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           1 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           2 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           3 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           4 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "+-------------+-----------------------+\n",
      "|   Iteration | End Ac. Consistency   |\n",
      "+=============+=======================+\n",
      "|           0 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           1 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           2 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           3 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           4 | 83.33%                |\n",
      "+-------------+-----------------------+\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
