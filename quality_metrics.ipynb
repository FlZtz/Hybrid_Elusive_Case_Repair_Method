{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation of Output Quality",
   "id": "373d3274ab50f5b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:51.194896Z",
     "start_time": "2024-05-06T17:38:48.335179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from config import extract_log_name, get_file_path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "7589758cab53053d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:51.811962Z",
     "start_time": "2024-05-06T17:38:51.796332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_elusive_equivalents(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add the elusive metrics to the metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the metrics.\n",
    "    :return: DataFrame containing the metrics with the elusive metrics added.\n",
    "    \"\"\"\n",
    "    start_completeness = metrics.loc[metrics['Iteration'] == 0, 'Completeness'].values[0]\n",
    "    \n",
    "    if start_completeness > 0:\n",
    "        for metric in [\"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\"]:\n",
    "            start_metric = metrics.loc[metrics['Iteration'] == 0, metric].values[0]\n",
    "            column_index = metrics.columns.get_loc(metric) + 1\n",
    "            metrics.insert(column_index, f\"Elus. {metric}\", None)\n",
    "\n",
    "            for i in metrics.index:\n",
    "                iteration_metric = metrics.loc[i, metric]\n",
    "                difference = iteration_metric - start_metric\n",
    "                added_metric = (difference / (100 - start_metric)) * 100\n",
    "                metrics.loc[i, f\"Elus. {metric}\"] = added_metric\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "c6aa4d0c7936acb4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:52.184363Z",
     "start_time": "2024-05-06T17:38:52.153382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_completely_correct_cases(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of completely correct Case ID values.\n",
    "\n",
    "    A completely correct Case ID value is one where the 'Case ID' matches the 'Determined Case ID' for all rows where\n",
    "    it appears, and vice versa.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = all(subset_correct['Case ID'] == subset_predicted[column])\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        subset_determined = predicted_df[predicted_df[column] == case_id]\n",
    "        subset_cases = correct_df.loc[subset_determined.index]\n",
    "\n",
    "        condition_2 = all(subset_determined[column] == subset_cases['Case ID'])\n",
    "\n",
    "        if condition_2:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    proportion_completely_correct = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_completely_correct\n",
    "\n",
    "\n",
    "def calculate_correct_case_different_naming(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str,\n",
    "                                            calculation: bool = True) -> Union[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually completely correct Case ID values.\n",
    "\n",
    "    A factually completely correct Case ID value is one where the Determined Case ID is different from the Case ID, yet\n",
    "    uniquely maps back to the same Case ID value.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :param calculation: If True, calculate the proportion of factually completely correct Case ID values.\n",
    "                        If False, return the list of factually completely correct Case ID values.\n",
    "    :return: Proportion of factually completely correct Case ID values if calculation is True,\n",
    "             otherwise return the DataFrame of factually completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0 if calculation else pd.DataFrame()\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = len(subset_predicted[column].unique()) == 1\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        unique_value = subset_predicted[column].iloc[0]\n",
    "\n",
    "        condition_2 = unique_value != case_id\n",
    "\n",
    "        if not condition_2:\n",
    "            continue\n",
    "\n",
    "        condition_3 = not predicted_df[(predicted_df[column] == unique_value) & \n",
    "                                       (correct_df['Case ID'] != case_id)].any().any()\n",
    "\n",
    "        if condition_3:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    if not calculation:\n",
    "        matching_rows = correct_df[correct_df['Case ID'].isin(correct_cases)]\n",
    "        return matching_rows\n",
    "\n",
    "    proportion_correct_different_naming = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_correct_different_naming\n",
    "\n",
    "\n",
    "def calculate_factual_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of factually matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = calculate_correct_case_different_naming(predicted_df, correct_df, column, False)\n",
    "    proportion_factual_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_factual_matching\n",
    "\n",
    "\n",
    "def calculate_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = predicted_df[predicted_df[column] == correct_df['Case ID']]\n",
    "    proportion_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_matching\n",
    "\n",
    "\n",
    "def evaluate_accuracy(predicted_df: pd.DataFrame, complete_df: pd.DataFrame, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the repaired log.\n",
    "\n",
    "    :param predicted_df: DataFrame containing the determined log.\n",
    "    :param complete_df: DataFrame containing complete log.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    matching = calculate_matching_proportion(predicted_df, complete_df, column)\n",
    "    factual_matching = calculate_factual_matching_proportion(predicted_df, complete_df, column)\n",
    "    correct_proportion = calculate_completely_correct_cases(predicted_df, complete_df, column)\n",
    "    factual_correct_proportion = calculate_correct_case_different_naming(predicted_df, complete_df, column)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": matching,\n",
    "        \"Factual Accuracy\": factual_matching,\n",
    "        \"Overall Accuracy\": matching + factual_matching,\n",
    "        \"Real Case Accuracy\": correct_proportion,\n",
    "        \"Factual Case Accuracy\": factual_correct_proportion,\n",
    "        \"Overall Case Accuracy\": correct_proportion + factual_correct_proportion\n",
    "    }\n"
   ],
   "id": "150b7690c957beff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:52.541529Z",
     "start_time": "2024-05-06T17:38:52.526009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_completeness(df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the completeness of the log.\n",
    "\n",
    "    :param df: DataFrame containing the log.\n",
    "    :param column: Column to evaluate completeness.\n",
    "    :return: Proportion of missing values in the 'Case ID' column.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return float('-inf')\n",
    "\n",
    "    percentage_not_na = (1 - df[column].isna().sum() / len(df[column])) * 100\n",
    "    return percentage_not_na\n"
   ],
   "id": "1d1fbcc9e37b07b9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:52.841976Z",
     "start_time": "2024-05-06T17:38:52.813126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_directly_following_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of cases containing solely correct directly following activities.\n",
    "\n",
    "    Solely correct directly following activities means that, for each case, it verifies whether the directly following \n",
    "    activities occur in the correct order and have the same non-zero number of predecessors and successors as defined \n",
    "    in the configuration.\n",
    "    \n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct directly following activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_directly_following = 0\n",
    "    directly_following = configuration['expert_input_values']['Directly Following']\n",
    "    always_directly_following = [pair for pair, occurrence in zip(\n",
    "        directly_following['values'], directly_following['occurrences']) if occurrence == 'always']\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            is_consistent_case = True\n",
    "            \n",
    "            for predecessor, successor in always_directly_following:\n",
    "                if not is_consistent_case:\n",
    "                    break\n",
    "                positions_predecessor = group[group['Activity'] == predecessor].index.tolist()\n",
    "                positions_successor = group[group['Activity'] == successor].index.tolist()\n",
    "\n",
    "                if not positions_predecessor or not positions_successor:\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "                \n",
    "                if len(positions_predecessor) != len(positions_successor):\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "                \n",
    "                first_predecessor, last_predecessor = positions_predecessor[0], positions_predecessor[-1]\n",
    "                first_successor, last_successor = positions_successor[0], positions_successor[-1]\n",
    "                \n",
    "                if first_predecessor > first_successor or last_predecessor > last_successor:\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "\n",
    "            if is_consistent_case:\n",
    "                num_correct_directly_following += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_directly_following / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def calculate_end_activity_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of correct end activities.\n",
    "\n",
    "    A correct end activity is one that is present in the expert input values for the end activity.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct end activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_end = 0\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            end_activity = group['Activity'].iloc[-1]\n",
    "            if end_activity in configuration['expert_input_values']['End Activity']['values']:\n",
    "                num_correct_end += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_end / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def calculate_start_activity_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of correct start activities.\n",
    "\n",
    "    A correct start activity is one that is present in the expert input values for the start activity.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct start activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_start = 0\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            start_activity = group['Activity'].iloc[0]\n",
    "            if start_activity in configuration['expert_input_values']['Start Activity']['values']:\n",
    "                num_correct_start += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_start / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def evaluate_consistency(df: pd.DataFrame, configuration: dict, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the consistency of the log.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    if not configuration['expert_input_attributes'] or column not in df.columns:\n",
    "        return {}\n",
    "    \n",
    "    start_activity_consistency, end_activity_consistency, directly_following_consistency = None, None, None\n",
    "\n",
    "    for attribute in configuration['expert_input_attributes']:\n",
    "        if attribute == 'Start Activity':\n",
    "            start_activity_consistency = calculate_start_activity_consistency(df, configuration, column)\n",
    "        elif attribute == 'End Activity':\n",
    "            end_activity_consistency = calculate_end_activity_consistency(df, configuration, column)\n",
    "        elif attribute == 'Directly Following':\n",
    "            directly_following_consistency = calculate_directly_following_consistency(df, configuration, column)\n",
    "\n",
    "    return {\n",
    "        \"St. Ac. Consistency\": start_activity_consistency,\n",
    "        \"End Ac. Consistency\": end_activity_consistency,\n",
    "        \"Di. Fo. Consistency\": directly_following_consistency\n",
    "    }\n"
   ],
   "id": "6de1ecf6abac8cab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:53.196611Z",
     "start_time": "2024-05-06T17:38:53.180904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_iteration(iteration: int, predicted_log: pd.DataFrame, complete_log: pd.DataFrame, configuration: dict, \n",
    "                       column: str = 'Determined Case ID') -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the quality metrics for a given iteration.\n",
    "    \n",
    "    :param iteration: The iteration number.\n",
    "    :param predicted_log: The DataFrame containing the predicted log.\n",
    "    :param complete_log: The DataFrame containing the complete log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate the metrics.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    completeness = evaluate_completeness(predicted_log, column)\n",
    "    iteration_metrics = {\"Iteration\": iteration, \"Completeness\": completeness}\n",
    "    accuracy = evaluate_accuracy(predicted_log, complete_log, column)\n",
    "    iteration_metrics.update(accuracy)\n",
    "    consistency = evaluate_consistency(predicted_log, configuration, column)\n",
    "    iteration_metrics.update(consistency)\n",
    "    return iteration_metrics\n"
   ],
   "id": "7718604500500e32",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:53.429406Z",
     "start_time": "2024-05-06T17:38:53.413770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_random_log(metrics: pd.DataFrame, random_log: pd.DataFrame, complete_log: pd.DataFrame, \n",
    "                        model_configuration: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the random log.\n",
    "    \n",
    "    :param metrics: The DataFrame containing the quality metrics.\n",
    "    :param random_log: The DataFrame containing the randomized log.\n",
    "    :param complete_log: The DataFrame containing the complete log.\n",
    "    :param model_configuration: The configuration dictionary.\n",
    "    :return: The DataFrame containing the quality metrics.\n",
    "    \"\"\"\n",
    "    random_metrics = metrics[metrics['Iteration'] == 0].copy()\n",
    "    iteration_metrics = evaluate_iteration(1, random_log, complete_log, model_configuration)\n",
    "    random_metrics = pd.concat([random_metrics, pd.DataFrame([iteration_metrics])], ignore_index=True)\n",
    "    return random_metrics\n"
   ],
   "id": "1b1fa2ebafd9d7a0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:53.649878Z",
     "start_time": "2024-05-06T17:38:53.633225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_repaired_logs(folder_path: str, log_name: str, complete_log: pd.DataFrame, \n",
    "                           configuration: dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the repaired logs.\n",
    "\n",
    "    :param folder_path: Path to the folder containing the repaired logs.\n",
    "    :param log_name: Name of the log.\n",
    "    :param complete_log: DataFrame containing the complete log.\n",
    "    :param configuration: Dictionary containing the expert input used for training.\n",
    "    :return: DataFrame containing the quality metrics and DataFrame containing the elusive log.\n",
    "    \"\"\"\n",
    "    quality_metrics = pd.DataFrame(columns=[\n",
    "        \"Iteration\", \"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\", \"Real Case Accuracy\",\n",
    "        \"Factual Case Accuracy\", \"Overall Case Accuracy\", \"St. Ac. Consistency\", \"End Ac. Consistency\", \n",
    "        \"Di. Fo. Consistency\"\n",
    "    ])\n",
    "    \n",
    "    elusive_log = pd.DataFrame()\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        num_iterations = len([f for f in os.listdir(folder_path) if f.endswith('.csv')\n",
    "                              and f.startswith(f\"determined_{log_name}_iteration_\")])\n",
    "\n",
    "        if num_iterations:\n",
    "            for i in range(1, num_iterations + 1):\n",
    "                log_file = os.path.join(folder_path, f\"determined_{log_name}_iteration_{i}.csv\")\n",
    "\n",
    "                if os.path.exists(log_file):\n",
    "                    predicted_log = pd.read_csv(log_file)\n",
    "\n",
    "                    if i == 1:\n",
    "                        iteration_metrics = evaluate_iteration(0, predicted_log, complete_log, configuration,\n",
    "                                                               'Original Case ID')\n",
    "                        quality_metrics = pd.DataFrame([iteration_metrics])\n",
    "                        elusive_log = predicted_log[['Original Case ID', 'Activity']]\n",
    "                    iteration_metrics = evaluate_iteration(i, predicted_log, complete_log, configuration)\n",
    "                    quality_metrics = pd.concat([quality_metrics, pd.DataFrame([iteration_metrics])], ignore_index=True)\n",
    "      \n",
    "    quality_metrics = format_consistency_metrics(quality_metrics, configuration)\n",
    "\n",
    "    return quality_metrics, elusive_log\n"
   ],
   "id": "15b37fa02ad6db9d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:53.822950Z",
     "start_time": "2024-05-06T17:38:53.808938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_consistency_metrics(metrics: pd.DataFrame, configuration: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the consistency metrics DataFrame.\n",
    "    \n",
    "    :param metrics: DataFrame containing the consistency metrics.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :return: DataFrame containing the consistency metrics with the appropriate columns.\n",
    "    \"\"\"\n",
    "    possible_expert_attributes = ['Start Activity', 'End Activity', 'Directly Following']\n",
    "    included_attributes = [attr for attr in possible_expert_attributes if \n",
    "                           attr in configuration['expert_input_attributes']]\n",
    "    missing_attributes = [attr for attr in possible_expert_attributes if \n",
    "                          attr not in configuration['expert_input_attributes']]\n",
    "    \n",
    "    if included_attributes:\n",
    "        if ('Directly Following' in included_attributes and \n",
    "                'always' not in configuration['expert_input_values']['Directly Following']['occurrences']):\n",
    "            metrics.drop(columns=\"Di. Fo. Consistency\", inplace=True)\n",
    "    \n",
    "    if missing_attributes:\n",
    "        if 'Start Activity' in missing_attributes:\n",
    "            metrics.drop(columns=\"St. Ac. Consistency\", inplace=True)\n",
    "        if 'End Activity' in missing_attributes:\n",
    "            metrics.drop(columns=\"End Ac. Consistency\", inplace=True)\n",
    "        if 'Directly Following' in missing_attributes:\n",
    "            metrics.drop(columns=\"Di. Fo. Consistency\", inplace=True)\n",
    "    \n",
    "    return metrics\n"
   ],
   "id": "542a104565063188",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:54.053603Z",
     "start_time": "2024-05-06T17:38:54.037946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_metrics(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the quality metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the quality metrics.\n",
    "    :return: Formatted DataFrame.\n",
    "    \"\"\"\n",
    "    if metrics.empty:\n",
    "        return metrics\n",
    "\n",
    "    cols_to_format = metrics.columns.drop('Iteration')\n",
    "    metrics[cols_to_format] = metrics[cols_to_format].applymap(lambda x: '{:.2f}%'.format(x) if pd.notnull(x) else \"\")\n",
    "    metrics['Iteration'] = metrics['Iteration'].astype(int)\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "d8d8f1a9cedfcf2d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:54.336280Z",
     "start_time": "2024-05-06T17:38:54.307074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_input() -> Tuple[pd.DataFrame, str, str, dict, Tokenizer]:\n",
    "    \"\"\"\n",
    "    Prompt the user to input the path to the folder containing the repaired logs as well as the complete log with \n",
    "    corresponding name and the tokenizer for the case IDs, and retrieve the model configuration if available.\n",
    "\n",
    "    :return: A tuple containing the complete log DataFrame, the name of the complete log file, the path to the folder \n",
    "     containing the repaired logs after each iteration, and a dictionary representing the model configuration if \n",
    "     available, otherwise an empty dictionary, and a Tokenizer object representing the tokenizer if available,\n",
    "     otherwise a Tokenizer object initialized with None.\n",
    "    \"\"\"\n",
    "    complete_log_path = get_file_path(\"preprocessed complete log\")\n",
    "\n",
    "    if os.path.exists(complete_log_path):\n",
    "        complete_log = pd.read_csv(complete_log_path)\n",
    "        print(\"CSV file successfully read.\")\n",
    "        log_name = extract_log_name(complete_log_path)\n",
    "\n",
    "        if \"DISPLAY\" in os.environ:\n",
    "            root = tk.Tk()\n",
    "            root.withdraw()\n",
    "\n",
    "            folder_path = filedialog.askdirectory(\n",
    "                title=\"Select the folder that contains the repaired logs after each iteration\")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "        else:\n",
    "            folder_path = input(\"Enter the path to the folder that contains the repaired logs after each iteration: \")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "\n",
    "            folder_path = folder_path.strip('\"')\n",
    "\n",
    "        print(\"Folder path successfully read.\")\n",
    "        \n",
    "        configuration_path = get_file_path(\"model configuration\")\n",
    "        \n",
    "        if os.path.exists(configuration_path):\n",
    "            with open(configuration_path, 'rb') as file:\n",
    "                model_configuration = pickle.load(file)\n",
    "                print(\"Model configuration file successfully read.\")\n",
    "            \n",
    "            model_configuration = {key: value.to_dict() if isinstance(value, pd.DataFrame) else value for key, value in\n",
    "                                   model_configuration.items()}\n",
    "            \n",
    "            tokenizer_path = get_file_path(\"case ID tokenizer\")\n",
    "            \n",
    "            if os.path.exists(tokenizer_path):\n",
    "                tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "                print(\"Tokenizer successfully read.\")\n",
    "                \n",
    "                return complete_log, log_name, folder_path, model_configuration, tokenizer\n",
    "\n",
    "            return complete_log, log_name, folder_path, model_configuration, Tokenizer(None)\n",
    "        \n",
    "        return complete_log, log_name, folder_path, {}, Tokenizer(None)\n",
    "\n",
    "    return pd.DataFrame(), \"\", \"\", {}, Tokenizer(None)\n"
   ],
   "id": "f635854ae114b1f4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:54.598637Z",
     "start_time": "2024-05-06T17:38:54.580830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_metrics(metrics: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print the evaluation metrics in a tabular format.\n",
    "    \n",
    "    :param metrics: DataFrame containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    num_columns = len(metrics.columns) - 1\n",
    "    num_tables = (num_columns + 3) // 4\n",
    "    \n",
    "    for i in range(num_tables):\n",
    "        start_idx = 1 + 4 * i\n",
    "        end_idx = min(start_idx + 4, num_columns + 1)\n",
    "        indices = [0] + list(range(start_idx, end_idx))\n",
    "        metrics_table = metrics.iloc[:, indices]\n",
    "        print(tabulate(metrics_table, headers='keys', tablefmt='grid', showindex=False))\n"
   ],
   "id": "57f4364067a174a4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:54.868090Z",
     "start_time": "2024-05-06T17:38:54.852315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def randomize_missing_case_ids(elusive_log: pd.DataFrame, tokenizer: Tokenizer) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomize the missing Case ID values in the elusive log.\n",
    "    \n",
    "    :param elusive_log: Log containing missing Case ID values.\n",
    "    :param tokenizer: Tokenizer object used to generate random case IDs.\n",
    "    :return: Log with missing Case ID values randomized.\n",
    "    \"\"\"\n",
    "    elusive_log['Determined Case ID'] = elusive_log['Original Case ID']\n",
    "    missing_indices = elusive_log['Determined Case ID'].isna()\n",
    "    \n",
    "    if missing_indices.any():\n",
    "        special_tokens = {\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"[NONE]\"}\n",
    "        all_tokens = set(tokenizer.get_vocab().keys())\n",
    "        eligible_tokens = list(all_tokens - special_tokens)\n",
    "        \n",
    "        if not eligible_tokens:\n",
    "            raise ValueError(\"Tokenizer vocabulary does not contain any eligible tokens.\")\n",
    "        \n",
    "        num_missing = missing_indices.sum()\n",
    "        random_tokens = np.random.choice(eligible_tokens, size=num_missing, replace=True)\n",
    "        \n",
    "        if num_missing != len(random_tokens):\n",
    "            raise ValueError(\"Could not generate enough unique random tokens.\")\n",
    "        \n",
    "        elusive_log.loc[missing_indices, 'Determined Case ID'] = random_tokens\n",
    "        \n",
    "    return elusive_log\n"
   ],
   "id": "14840cf9ee8f2eb8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:38:55.397103Z",
     "start_time": "2024-05-06T17:38:55.381008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_metrics(metrics: pd.DataFrame, log_name: str, model_name: str = 'transformer',\n",
    "                 log: pd.DataFrame = pd.DataFrame()) -> None:\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to a CSV file and save the log to a CSV file if applicable.\n",
    "    \n",
    "    :param metrics: DataFrame containing evaluation metrics.\n",
    "    :param log_name: Name of the log file.\n",
    "    :param model_name: Name of the model used for evaluation. Default is 'transformer'.\n",
    "    :param log: DataFrame containing the log. Default is an empty DataFrame.\n",
    "    \"\"\"\n",
    "    folder_name = f\"evaluation/{log_name}/{model_name}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    current_time = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    file_name = f'metrics_{current_time}.csv'\n",
    "    metrics.to_csv(os.path.join(folder_name, file_name), index=False)\n",
    "    \n",
    "    if model_name == 'randomized':\n",
    "        file_name = f'randomized_{log_name}.csv'\n",
    "        log.to_csv(os.path.join(folder_name, file_name), index=False)\n"
   ],
   "id": "2b88746e93f1974d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T17:39:22.901565Z",
     "start_time": "2024-05-06T17:38:55.902393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "complete_log, log_name, folder_path, model_configuration, tokenizer = get_input()\n",
    "\n",
    "if folder_path:\n",
    "    metrics, elusive_log = evaluate_repaired_logs(folder_path, log_name, complete_log, model_configuration)\n",
    "    random_log = randomize_missing_case_ids(elusive_log, tokenizer)\n",
    "    random_metrics = evaluate_random_log(metrics, random_log, complete_log, model_configuration)\n",
    "    random_metrics = format_consistency_metrics(random_metrics, model_configuration)\n",
    "    metrics = add_elusive_equivalents(metrics)\n",
    "    random_metrics = add_elusive_equivalents(random_metrics)\n",
    "    metrics = format_metrics(metrics)\n",
    "    random_metrics = format_metrics(random_metrics)\n",
    "    save_metrics(metrics, log_name)\n",
    "    save_metrics(random_metrics, log_name, 'randomized', random_log)\n",
    "    print(\"\\n=== TRANSFORMER ===\\n\")\n",
    "    print_metrics(metrics)\n",
    "    print(\"\\n=== RANDOMIZED ===\\n\")\n",
    "    print_metrics(random_metrics)\n"
   ],
   "id": "8364b4c7b44adda0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully read.\n",
      "Folder path successfully read.\n",
      "Model configuration file successfully read.\n",
      "Tokenizer successfully read.\n",
      "\n",
      "=== TRANSFORMER ===\n",
      "\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         | 0.00%                | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 92.86%         | 62.50%               | 83.33%     | 12.50%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           2 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           3 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           4 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 83.33%             | 12.50%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           2 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           3 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           4 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|   Iteration | Real Case Accuracy   | Factual Case Accuracy   | Overall Case Accuracy   | St. Ac. Consistency   |\n",
      "+=============+======================+=========================+=========================+=======================+\n",
      "|           0 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           1 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           2 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           3 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           4 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "+-------------+-----------------------+\n",
      "|   Iteration | End Ac. Consistency   |\n",
      "+=============+=======================+\n",
      "|           0 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           1 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           2 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           3 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           4 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "\n",
      "=== RANDOMIZED ===\n",
      "\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         | 0.00%                | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 100.00%        | 100.00%              | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|   Iteration | Real Case Accuracy   | Factual Case Accuracy   | Overall Case Accuracy   | St. Ac. Consistency   |\n",
      "+=============+======================+=========================+=========================+=======================+\n",
      "|           0 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           1 | 0.00%                | 0.00%                   | 0.00%                   | 50.00%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "+-------------+-----------------------+\n",
      "|   Iteration | End Ac. Consistency   |\n",
      "+=============+=======================+\n",
      "|           0 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           1 | 50.00%                |\n",
      "+-------------+-----------------------+\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
