{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:39.912554Z",
     "start_time": "2024-04-19T16:39:34.572827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "from config import extract_log_name, get_file_path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "7589758cab53053d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:39.944750Z",
     "start_time": "2024-04-19T16:39:39.918708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_elusive_equivalents(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add the elusive metrics to the metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the metrics.\n",
    "    :return: DataFrame containing the metrics with the elusive metrics added.\n",
    "    \"\"\"\n",
    "    start_completeness = metrics.loc[metrics['Iteration'] == 0, 'Completeness'].values[0]\n",
    "    if start_completeness > 0:\n",
    "        for metric in [\"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\", \"Real Case Accuracy\",\n",
    "                       \"Factual Case Accuracy\", \"Overall Case Accuracy\"]:\n",
    "            start_metric = metrics.loc[metrics['Iteration'] == 0, metric].values[0]\n",
    "            \n",
    "            column_index = metrics.columns.get_loc(metric) + 1\n",
    "            metrics.insert(column_index, f\"Elus. {metric}\", None)\n",
    "\n",
    "            for i in metrics.loc[metrics['Iteration'] > 0].index:\n",
    "                iteration_metric = metrics.loc[i, metric]\n",
    "                difference = iteration_metric - start_metric\n",
    "                added_metric = (difference / (100 - start_metric)) * 100\n",
    "                metrics.loc[i, f\"Elus. {metric}\"] = added_metric\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "c6aa4d0c7936acb4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:39.991898Z",
     "start_time": "2024-04-19T16:39:39.945698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_completely_correct_cases(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of completely correct Case ID values.\n",
    "\n",
    "    A completely correct Case ID value is one where the 'Case ID' matches the 'Determined Case ID' for all rows where\n",
    "    it appears, and vice versa.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = all(subset_correct['Case ID'] == subset_predicted[column])\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        subset_determined = predicted_df[predicted_df[column] == case_id]\n",
    "        subset_cases = correct_df.loc[subset_determined.index]\n",
    "\n",
    "        condition_2 = all(subset_determined[column] == subset_cases['Case ID'])\n",
    "\n",
    "        if condition_2:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    proportion_completely_correct = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_completely_correct\n",
    "\n",
    "\n",
    "def calculate_correct_case_different_naming(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str,\n",
    "                                            calculation: bool = True) -> Union[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually completely correct Case ID values.\n",
    "\n",
    "    A factually completely correct Case ID value is one where the Determined Case ID is different from the Case ID, yet\n",
    "    uniquely maps back to the same Case ID value.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :param calculation: If True, calculate the proportion of factually completely correct Case ID values.\n",
    "                        If False, return the list of factually completely correct Case ID values.\n",
    "    :return: Proportion of factually completely correct Case ID values if calculation is True,\n",
    "             otherwise return the DataFrame of factually completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0 if calculation else pd.DataFrame()\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = len(subset_predicted[column].unique()) == 1\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        unique_value = subset_predicted[column].iloc[0]\n",
    "\n",
    "        condition_2 = unique_value != case_id\n",
    "\n",
    "        if not condition_2:\n",
    "            continue\n",
    "\n",
    "        condition_3 = not predicted_df[(predicted_df[column] == unique_value) & \n",
    "                                       (correct_df['Case ID'] != case_id)].any().any()\n",
    "\n",
    "        if condition_3:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    if not calculation:\n",
    "        matching_rows = correct_df[correct_df['Case ID'].isin(correct_cases)]\n",
    "        return matching_rows\n",
    "\n",
    "    proportion_correct_different_naming = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_correct_different_naming\n",
    "\n",
    "\n",
    "def calculate_factual_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of factually matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = calculate_correct_case_different_naming(predicted_df, correct_df, column, False)\n",
    "    proportion_factual_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_factual_matching\n",
    "\n",
    "\n",
    "def calculate_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = predicted_df[predicted_df[column] == correct_df['Case ID']]\n",
    "    proportion_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_matching\n",
    "\n",
    "\n",
    "def evaluate_accuracy(predicted_df: pd.DataFrame, complete_df: pd.DataFrame, \n",
    "                      column: str = 'Determined Case ID') -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the repaired log.\n",
    "\n",
    "    :param predicted_df: DataFrame containing the determined log.\n",
    "    :param complete_df: DataFrame containing complete log.\n",
    "    :param column: Column to evaluate accuracy. Default is 'Determined Case ID'.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    matching = calculate_matching_proportion(predicted_df, complete_df, column)\n",
    "    factual_matching = calculate_factual_matching_proportion(predicted_df, complete_df, column)\n",
    "    correct_proportion = calculate_completely_correct_cases(predicted_df, complete_df, column)\n",
    "    factual_correct_proportion = calculate_correct_case_different_naming(predicted_df, complete_df, column)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": matching,\n",
    "        \"Factual Accuracy\": factual_matching,\n",
    "        \"Overall Accuracy\": matching + factual_matching,\n",
    "        \"Real Case Accuracy\": correct_proportion,\n",
    "        \"Factual Case Accuracy\": factual_correct_proportion,\n",
    "        \"Overall Case Accuracy\": correct_proportion + factual_correct_proportion\n",
    "    }\n"
   ],
   "id": "150b7690c957beff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:40.023561Z",
     "start_time": "2024-04-19T16:39:39.998464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_completeness(df: pd.DataFrame, column: str = 'Determined Case ID') -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the completeness of the log.\n",
    "\n",
    "    :param df: DataFrame containing the log.\n",
    "    :param column: Column to evaluate completeness. Default is 'Determined Case ID'.\n",
    "    :return: Proportion of missing values in the 'Case ID' column.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return float('-inf')\n",
    "\n",
    "    percentage_not_na = (1 - df[column].isna().sum() / len(df[column])) * 100\n",
    "    return percentage_not_na\n"
   ],
   "id": "1d1fbcc9e37b07b9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:40.055097Z",
     "start_time": "2024-04-19T16:39:40.023561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_repaired_logs(folder_path: str, log_name: str, complete_log: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the repaired logs.\n",
    "\n",
    "    :param folder_path: Path to the folder containing the repaired logs.\n",
    "    :param log_name: Name of the log.\n",
    "    :param complete_log: DataFrame containing the complete log.\n",
    "    :return: DataFrame containing the quality metrics.\n",
    "    \"\"\"\n",
    "    quality_metrics = pd.DataFrame(columns=[\n",
    "        \"Iteration\", \"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\", \"Real Case Accuracy\",\n",
    "        \"Factual Case Accuracy\", \"Overall Case Accuracy\"\n",
    "    ])\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        num_iterations = len([f for f in os.listdir(folder_path) if f.endswith('.csv')\n",
    "                              and f.startswith(f\"determined_{log_name}_iteration_\")])\n",
    "\n",
    "        if num_iterations:\n",
    "            for i in range(1, num_iterations + 1):\n",
    "                log_file = os.path.join(folder_path, f\"determined_{log_name}_iteration_{i}.csv\")\n",
    "\n",
    "                if os.path.exists(log_file):\n",
    "                    predicted_log = pd.read_csv(log_file)\n",
    "\n",
    "                    if i == 1:\n",
    "                        completeness = evaluate_completeness(predicted_log, 'Original Case ID')\n",
    "                        iteration_metrics = {\"Iteration\": 0, \"Completeness\": completeness}\n",
    "                        accuracy = evaluate_accuracy(predicted_log, complete_log, 'Original Case ID')\n",
    "                        iteration_metrics.update(accuracy)\n",
    "                        quality_metrics = pd.DataFrame([iteration_metrics])\n",
    "                    completeness = evaluate_completeness(predicted_log)\n",
    "                    iteration_metrics = {\"Iteration\": i, \"Completeness\": completeness}\n",
    "                    accuracy = evaluate_accuracy(predicted_log, complete_log)\n",
    "                    iteration_metrics.update(accuracy)\n",
    "                    quality_metrics = pd.concat([quality_metrics, pd.DataFrame([iteration_metrics])], ignore_index=True)\n",
    "\n",
    "    return quality_metrics\n"
   ],
   "id": "15b37fa02ad6db9d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:40.086671Z",
     "start_time": "2024-04-19T16:39:40.060381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_metrics(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the quality metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the quality metrics.\n",
    "    :return: Formatted DataFrame.\n",
    "    \"\"\"\n",
    "    if metrics.empty:\n",
    "        return metrics\n",
    "\n",
    "    cols_to_format = metrics.columns.drop('Iteration')\n",
    "    metrics[cols_to_format] = metrics[cols_to_format].applymap(lambda x: '{:.2f}%'.format(x) if pd.notnull(x) else \"\")\n",
    "    metrics['Iteration'] = metrics['Iteration'].astype(int)\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "d8d8f1a9cedfcf2d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:40.102417Z",
     "start_time": "2024-04-19T16:39:40.089683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_input() -> Tuple[pd.DataFrame, str, str]:\n",
    "    \"\"\"\n",
    "    Get the path to the folder containing the repaired logs as well as the complete log with corresponding name.\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    complete_log_path = get_file_path(\"preprocessed complete log\")\n",
    "\n",
    "    if os.path.exists(complete_log_path):\n",
    "        complete_log = pd.read_csv(complete_log_path)\n",
    "        print(\"CSV file successfully read.\")\n",
    "        log_name = extract_log_name(complete_log_path)\n",
    "\n",
    "        if \"DISPLAY\" in os.environ:\n",
    "            root = tk.Tk()\n",
    "            root.withdraw()\n",
    "\n",
    "            folder_path = filedialog.askdirectory(\n",
    "                title=\"Select the folder that contains the repaired logs after each iteration\")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "        else:\n",
    "            folder_path = input(\"Enter the path to the folder that contains the repaired logs after each iteration: \")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "\n",
    "            folder_path = folder_path.strip('\"')\n",
    "\n",
    "        print(\"Folder path successfully read.\")\n",
    "\n",
    "        return complete_log, log_name, folder_path\n",
    "\n",
    "    return pd.DataFrame(), \"\", \"\"\n"
   ],
   "id": "f635854ae114b1f4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:40.118484Z",
     "start_time": "2024-04-19T16:39:40.102417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_metrics(metrics: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print the evaluation metrics in a tabular format.\n",
    "    \n",
    "    :param metrics: DataFrame containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    num_columns = len(metrics.columns) - 1\n",
    "    num_tables = (num_columns + 3) // 4\n",
    "    \n",
    "    for i in range(num_tables):\n",
    "        start_idx = 1 + 4 * i\n",
    "        end_idx = min(start_idx + 4, num_columns + 1)\n",
    "        indices = [0] + list(range(start_idx, end_idx))\n",
    "        metrics_table = metrics.iloc[:, indices]\n",
    "        print(tabulate(metrics_table, headers='keys', tablefmt='grid', showindex=False))\n"
   ],
   "id": "57f4364067a174a4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:40.134237Z",
     "start_time": "2024-04-19T16:39:40.119218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_metrics(metrics: pd.DataFrame, log_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to a CSV file.\n",
    "    \n",
    "    :param metrics: DataFrame containing evaluation metrics.\n",
    "    :param log_name: Name of the log file.\n",
    "    \"\"\"\n",
    "    folder_name = f\"evaluation/{log_name}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    current_time = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    file_name = f'metrics_{current_time}.csv'\n",
    "    metrics.to_csv(os.path.join(folder_name, file_name), index=False)\n"
   ],
   "id": "2b88746e93f1974d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T16:39:57.981277Z",
     "start_time": "2024-04-19T16:39:40.134237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "complete_log, log_name, folder_path = get_input()\n",
    "\n",
    "if folder_path:\n",
    "    metrics = evaluate_repaired_logs(folder_path, log_name, complete_log)\n",
    "    metrics = add_elusive_equivalents(metrics)\n",
    "    metrics = format_metrics(metrics)\n",
    "    save_metrics(metrics, log_name)\n",
    "    print_metrics(metrics)\n"
   ],
   "id": "8364b4c7b44adda0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully read.\n",
      "Folder path successfully read.\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         |                      | 80.95%     |                  |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 97.62%         | 87.50%               | 83.33%     | 12.50%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           2 | 97.62%         | 87.50%               | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              |                          | 80.95%             |                          |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 83.33%             | 12.50%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           2 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+----------------------------+-------------------------+-------------------------------+\n",
      "|   Iteration | Real Case Accuracy   | Elus. Real Case Accuracy   | Factual Case Accuracy   | Elus. Factual Case Accuracy   |\n",
      "+=============+======================+============================+=========================+===============================+\n",
      "|           0 | 33.33%               |                            | 0.00%                   |                               |\n",
      "+-------------+----------------------+----------------------------+-------------------------+-------------------------------+\n",
      "|           1 | 33.33%               | 0.00%                      | 0.00%                   | 0.00%                         |\n",
      "+-------------+----------------------+----------------------------+-------------------------+-------------------------------+\n",
      "|           2 | 16.67%               | -25.00%                    | 0.00%                   | 0.00%                         |\n",
      "+-------------+----------------------+----------------------------+-------------------------+-------------------------------+\n",
      "+-------------+-------------------------+-------------------------------+\n",
      "|   Iteration | Overall Case Accuracy   | Elus. Overall Case Accuracy   |\n",
      "+=============+=========================+===============================+\n",
      "|           0 | 33.33%                  |                               |\n",
      "+-------------+-------------------------+-------------------------------+\n",
      "|           1 | 33.33%                  | 0.00%                         |\n",
      "+-------------+-------------------------+-------------------------------+\n",
      "|           2 | 16.67%                  | -25.00%                       |\n",
      "+-------------+-------------------------+-------------------------------+\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
