{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation of Output Quality",
   "id": "373d3274ab50f5b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.510820Z",
     "start_time": "2024-05-07T12:18:53.739618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tabulate import tabulate\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from config import extract_log_name, get_file_path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "7589758cab53053d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.542033Z",
     "start_time": "2024-05-07T12:18:56.510820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_elusive_equivalents(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add the elusive metrics to the metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the metrics.\n",
    "    :return: DataFrame containing the metrics with the elusive metrics added.\n",
    "    \"\"\"\n",
    "    start_completeness = metrics.loc[metrics['Iteration'] == 0, 'Completeness'].values[0]\n",
    "    \n",
    "    if start_completeness > 0:\n",
    "        for metric in [\"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\"]:\n",
    "            start_metric = metrics.loc[metrics['Iteration'] == 0, metric].values[0]\n",
    "            column_index = metrics.columns.get_loc(metric) + 1\n",
    "            metrics.insert(column_index, f\"Elus. {metric}\", None)\n",
    "\n",
    "            for i in metrics.index:\n",
    "                iteration_metric = metrics.loc[i, metric]\n",
    "                difference = iteration_metric - start_metric\n",
    "                added_metric = (difference / (100 - start_metric)) * 100\n",
    "                metrics.loc[i, f\"Elus. {metric}\"] = added_metric\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "c6aa4d0c7936acb4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.573245Z",
     "start_time": "2024-05-07T12:18:56.542033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_completely_correct_cases(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of completely correct Case ID values.\n",
    "\n",
    "    A completely correct Case ID value is one where the 'Case ID' matches the 'Determined Case ID' for all rows where\n",
    "    it appears, and vice versa.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = all(subset_correct['Case ID'] == subset_predicted[column])\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        subset_determined = predicted_df[predicted_df[column] == case_id]\n",
    "        subset_cases = correct_df.loc[subset_determined.index]\n",
    "\n",
    "        condition_2 = all(subset_determined[column] == subset_cases['Case ID'])\n",
    "\n",
    "        if condition_2:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    proportion_completely_correct = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_completely_correct\n",
    "\n",
    "\n",
    "def calculate_correct_case_different_naming(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str,\n",
    "                                            calculation: bool = True) -> Union[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually completely correct Case ID values.\n",
    "\n",
    "    A factually completely correct Case ID value is one where the Determined Case ID is different from the Case ID, yet\n",
    "    uniquely maps back to the same Case ID value.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :param calculation: If True, calculate the proportion of factually completely correct Case ID values.\n",
    "                        If False, return the list of factually completely correct Case ID values.\n",
    "    :return: Proportion of factually completely correct Case ID values if calculation is True,\n",
    "             otherwise return the DataFrame of factually completely correct Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0 if calculation else pd.DataFrame()\n",
    "\n",
    "    correct_cases = []\n",
    "\n",
    "    for case_id in correct_df['Case ID'].unique():\n",
    "        subset_correct = correct_df[correct_df['Case ID'] == case_id]\n",
    "        subset_predicted = predicted_df.loc[subset_correct.index]\n",
    "\n",
    "        condition_1 = len(subset_predicted[column].unique()) == 1\n",
    "\n",
    "        if not condition_1:\n",
    "            continue\n",
    "\n",
    "        unique_value = subset_predicted[column].iloc[0]\n",
    "\n",
    "        condition_2 = unique_value != case_id\n",
    "\n",
    "        if not condition_2:\n",
    "            continue\n",
    "\n",
    "        condition_3 = not predicted_df[(predicted_df[column] == unique_value) & \n",
    "                                       (correct_df['Case ID'] != case_id)].any().any()\n",
    "\n",
    "        if condition_3:\n",
    "            correct_cases.append(case_id)\n",
    "\n",
    "    if not calculation:\n",
    "        matching_rows = correct_df[correct_df['Case ID'].isin(correct_cases)]\n",
    "        return matching_rows\n",
    "\n",
    "    proportion_correct_different_naming = len(correct_cases) / len(correct_df['Case ID'].unique()) * 100\n",
    "\n",
    "    return proportion_correct_different_naming\n",
    "\n",
    "\n",
    "def calculate_factual_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of factually matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of factually matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = calculate_correct_case_different_naming(predicted_df, correct_df, column, False)\n",
    "    proportion_factual_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_factual_matching\n",
    "\n",
    "\n",
    "def calculate_matching_proportion(predicted_df: pd.DataFrame, correct_df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of matching Case ID values.\n",
    "\n",
    "    :param predicted_df: DataFrame containing Determined Case ID values.\n",
    "    :param correct_df: DataFrame containing Case ID values.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Proportion of matching Case ID values.\n",
    "    \"\"\"\n",
    "    if predicted_df.empty or correct_df.empty:\n",
    "        return 0\n",
    "\n",
    "    matching_rows = predicted_df[predicted_df[column] == correct_df['Case ID']]\n",
    "    proportion_matching = len(matching_rows) / len(predicted_df) * 100\n",
    "\n",
    "    return proportion_matching\n",
    "\n",
    "\n",
    "def evaluate_accuracy(predicted_df: pd.DataFrame, complete_df: pd.DataFrame, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the repaired log.\n",
    "\n",
    "    :param predicted_df: DataFrame containing the determined log.\n",
    "    :param complete_df: DataFrame containing complete log.\n",
    "    :param column: Column to evaluate accuracy.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    matching = calculate_matching_proportion(predicted_df, complete_df, column)\n",
    "    factual_matching = calculate_factual_matching_proportion(predicted_df, complete_df, column)\n",
    "    correct_proportion = calculate_completely_correct_cases(predicted_df, complete_df, column)\n",
    "    factual_correct_proportion = calculate_correct_case_different_naming(predicted_df, complete_df, column)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": matching,\n",
    "        \"Factual Accuracy\": factual_matching,\n",
    "        \"Overall Accuracy\": matching + factual_matching,\n",
    "        \"Real Case Accuracy\": correct_proportion,\n",
    "        \"Factual Case Accuracy\": factual_correct_proportion,\n",
    "        \"Overall Case Accuracy\": correct_proportion + factual_correct_proportion\n",
    "    }\n"
   ],
   "id": "150b7690c957beff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.589070Z",
     "start_time": "2024-05-07T12:18:56.573245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_completeness(df: pd.DataFrame, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the completeness of the log.\n",
    "\n",
    "    :param df: DataFrame containing the log.\n",
    "    :param column: Column to evaluate completeness.\n",
    "    :return: Proportion of missing values in the 'Case ID' column.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return float('-inf')\n",
    "\n",
    "    percentage_not_na = (1 - df[column].isna().sum() / len(df[column])) * 100\n",
    "    return percentage_not_na\n"
   ],
   "id": "1d1fbcc9e37b07b9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.620533Z",
     "start_time": "2024-05-07T12:18:56.589070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_directly_following_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of cases containing solely correct directly following activities.\n",
    "\n",
    "    Solely correct directly following activities means that, for each case, it verifies whether the directly following \n",
    "    activities occur in the correct order and have the same non-zero number of predecessors and successors as defined \n",
    "    in the configuration.\n",
    "    \n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct directly following activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_directly_following = 0\n",
    "    directly_following = configuration['expert_input_values']['Directly Following']\n",
    "    always_directly_following = [pair for pair, occurrence in zip(\n",
    "        directly_following['values'], directly_following['occurrences']) if occurrence == 'always']\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            is_consistent_case = True\n",
    "            \n",
    "            for predecessor, successor in always_directly_following:\n",
    "                if not is_consistent_case:\n",
    "                    break\n",
    "                positions_predecessor = group[group['Activity'] == predecessor].index.tolist()\n",
    "                positions_successor = group[group['Activity'] == successor].index.tolist()\n",
    "\n",
    "                if not positions_predecessor or not positions_successor:\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "                \n",
    "                if len(positions_predecessor) != len(positions_successor):\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "                \n",
    "                first_predecessor, last_predecessor = positions_predecessor[0], positions_predecessor[-1]\n",
    "                first_successor, last_successor = positions_successor[0], positions_successor[-1]\n",
    "                \n",
    "                if first_predecessor > first_successor or last_predecessor > last_successor:\n",
    "                    is_consistent_case = False\n",
    "                    break\n",
    "\n",
    "            if is_consistent_case:\n",
    "                num_correct_directly_following += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_directly_following / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def calculate_end_activity_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of correct end activities.\n",
    "\n",
    "    A correct end activity is one that is present in the expert input values for the end activity.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct end activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_end = 0\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            end_activity = group['Activity'].iloc[-1]\n",
    "            if end_activity in configuration['expert_input_values']['End Activity']['values']:\n",
    "                num_correct_end += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_end / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def calculate_start_activity_consistency(df: pd.DataFrame, configuration: dict, column: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of correct start activities.\n",
    "\n",
    "    A correct start activity is one that is present in the expert input values for the start activity.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: The proportion of correct start activities in all cases.\n",
    "    \"\"\"\n",
    "    num_correct_start = 0\n",
    "\n",
    "    for case_id, group in df.groupby(column):\n",
    "        if pd.notna(case_id):\n",
    "            start_activity = group['Activity'].iloc[0]\n",
    "            if start_activity in configuration['expert_input_values']['Start Activity']['values']:\n",
    "                num_correct_start += 1\n",
    "\n",
    "    num_cases = len(df[column].dropna().unique())\n",
    "\n",
    "    return (num_correct_start / num_cases * 100) if num_cases else 0\n",
    "\n",
    "\n",
    "def evaluate_consistency(df: pd.DataFrame, configuration: dict, column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the consistency of the log.\n",
    "\n",
    "    :param df: The DataFrame containing the log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate consistency.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    if not configuration['expert_input_attributes'] or column not in df.columns:\n",
    "        return {}\n",
    "    \n",
    "    start_activity_consistency, end_activity_consistency, directly_following_consistency = None, None, None\n",
    "\n",
    "    for attribute in configuration['expert_input_attributes']:\n",
    "        if attribute == 'Start Activity':\n",
    "            start_activity_consistency = calculate_start_activity_consistency(df, configuration, column)\n",
    "        elif attribute == 'End Activity':\n",
    "            end_activity_consistency = calculate_end_activity_consistency(df, configuration, column)\n",
    "        elif attribute == 'Directly Following':\n",
    "            directly_following_consistency = calculate_directly_following_consistency(df, configuration, column)\n",
    "\n",
    "    return {\n",
    "        \"St. Ac. Consistency\": start_activity_consistency,\n",
    "        \"End Ac. Consistency\": end_activity_consistency,\n",
    "        \"Di. Fo. Consistency\": directly_following_consistency\n",
    "    }\n"
   ],
   "id": "6de1ecf6abac8cab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.636249Z",
     "start_time": "2024-05-07T12:18:56.620533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_iteration(iteration: int, predicted_log: pd.DataFrame, complete_log: pd.DataFrame, configuration: dict, \n",
    "                       column: str = 'Determined Case ID') -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the quality metrics for a given iteration.\n",
    "    \n",
    "    :param iteration: The iteration number.\n",
    "    :param predicted_log: The DataFrame containing the predicted log.\n",
    "    :param complete_log: The DataFrame containing the complete log.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :param column: The column to evaluate the metrics.\n",
    "    :return: Dictionary containing the quality metrics.\n",
    "    \"\"\"\n",
    "    completeness = evaluate_completeness(predicted_log, column)\n",
    "    iteration_metrics = {\"Iteration\": iteration, \"Completeness\": completeness}\n",
    "    accuracy = evaluate_accuracy(predicted_log, complete_log, column)\n",
    "    iteration_metrics.update(accuracy)\n",
    "    consistency = evaluate_consistency(predicted_log, configuration, column)\n",
    "    iteration_metrics.update(consistency)\n",
    "    return iteration_metrics\n"
   ],
   "id": "7718604500500e32",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.652295Z",
     "start_time": "2024-05-07T12:18:56.636249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_other_log(metrics: pd.DataFrame, log: pd.DataFrame, complete_log: pd.DataFrame,\n",
    "                       model_configuration: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the created log.\n",
    "    \n",
    "    :param metrics: The DataFrame containing the quality metrics.\n",
    "    :param log: The DataFrame containing the created log.\n",
    "    :param complete_log: The DataFrame containing the complete log.\n",
    "    :param model_configuration: The configuration dictionary.\n",
    "    :return: The DataFrame containing the quality metrics.\n",
    "    \"\"\"\n",
    "    log_metrics = metrics[metrics['Iteration'] == 0].copy()\n",
    "    iteration_metrics = evaluate_iteration(1, log, complete_log, model_configuration)\n",
    "    log_metrics = pd.concat([log_metrics, pd.DataFrame([iteration_metrics])], ignore_index=True)\n",
    "    return log_metrics\n"
   ],
   "id": "1b1fa2ebafd9d7a0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.667960Z",
     "start_time": "2024-05-07T12:18:56.652295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_repaired_logs(folder_path: str, log_name: str, complete_log: pd.DataFrame, \n",
    "                           configuration: dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the repaired logs.\n",
    "\n",
    "    :param folder_path: Path to the folder containing the repaired logs.\n",
    "    :param log_name: Name of the log.\n",
    "    :param complete_log: DataFrame containing the complete log.\n",
    "    :param configuration: Dictionary containing the expert input used for training.\n",
    "    :return: DataFrame containing the quality metrics and DataFrame containing the elusive log.\n",
    "    \"\"\"\n",
    "    quality_metrics = pd.DataFrame(columns=[\n",
    "        \"Iteration\", \"Completeness\", \"Accuracy\", \"Factual Accuracy\", \"Overall Accuracy\", \"Real Case Accuracy\",\n",
    "        \"Factual Case Accuracy\", \"Overall Case Accuracy\", \"St. Ac. Consistency\", \"End Ac. Consistency\", \n",
    "        \"Di. Fo. Consistency\"\n",
    "    ])\n",
    "    \n",
    "    elusive_log = pd.DataFrame()\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        num_iterations = len([f for f in os.listdir(folder_path) if f.endswith('.csv')\n",
    "                              and f.startswith(f\"determined_{log_name}_iteration_\")])\n",
    "\n",
    "        if num_iterations:\n",
    "            for i in range(1, num_iterations + 1):\n",
    "                log_file = os.path.join(folder_path, f\"determined_{log_name}_iteration_{i}.csv\")\n",
    "\n",
    "                if os.path.exists(log_file):\n",
    "                    predicted_log = pd.read_csv(log_file)\n",
    "\n",
    "                    if i == 1:\n",
    "                        iteration_metrics = evaluate_iteration(0, predicted_log, complete_log, configuration,\n",
    "                                                               'Original Case ID')\n",
    "                        quality_metrics = pd.DataFrame([iteration_metrics])\n",
    "                        columns_to_exclude = {'Determined Case ID', 'Iteration Probability',\n",
    "                                              'Determination Probability', 'Determination Follow-up Probability'}\n",
    "                        elusive_log = predicted_log.drop(columns=columns_to_exclude.intersection(predicted_log.columns))\n",
    "                    iteration_metrics = evaluate_iteration(i, predicted_log, complete_log, configuration)\n",
    "                    quality_metrics = pd.concat([quality_metrics, pd.DataFrame([iteration_metrics])], ignore_index=True)\n",
    "      \n",
    "    quality_metrics = format_consistency_metrics(quality_metrics, configuration)\n",
    "\n",
    "    return quality_metrics, elusive_log\n"
   ],
   "id": "15b37fa02ad6db9d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.683562Z",
     "start_time": "2024-05-07T12:18:56.667960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fill_missing_case_ids_logreg(complete_log: pd.DataFrame, elusive_log: pd.DataFrame, log_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill the missing Case ID values in the elusive log using a logistic regression model.\n",
    "    \n",
    "    :param complete_log: The DataFrame containing the complete log.\n",
    "    :param elusive_log: The DataFrame containing the elusive log.\n",
    "    :param log_name: The name of the log file.\n",
    "    :return: The DataFrame containing the elusive log with the missing Case ID values filled.\n",
    "    \"\"\"\n",
    "    log_reg_file = f\"evaluation/{log_name}/logistic_regression/logistic_regression_{log_name}.csv\"\n",
    "    \n",
    "    if os.path.exists(log_reg_file) and os.path.getsize(log_reg_file) > 0:\n",
    "        log = pd.read_csv(log_reg_file)\n",
    "        print(\"Logistic regression log successfully read.\")\n",
    "        return log\n",
    "    \n",
    "    elusive_log['Determined Case ID'] = elusive_log['Original Case ID']\n",
    "    input_data = elusive_log.copy()\n",
    "    \n",
    "    categorical_cols = ['Activity', 'Resource']\n",
    "    for col in categorical_cols:\n",
    "        if col in input_data.columns:\n",
    "            if len(input_data[col].unique()) > 2:\n",
    "                encoder = OneHotEncoder(sparse=False)\n",
    "                encoded_cols = pd.DataFrame(encoder.fit_transform(input_data[[col]]),\n",
    "                                            columns=encoder.get_feature_names_out([col]))\n",
    "                input_data.drop(columns=[col], inplace=True)\n",
    "                input_data = pd.concat([input_data, encoded_cols], axis=1)\n",
    "            else:\n",
    "                input_data[col] = input_data[col].astype('category').cat.codes\n",
    "    \n",
    "    if 'Timestamp' in input_data.columns:\n",
    "        input_data['Timestamp'] = pd.to_datetime(input_data['Timestamp'])\n",
    "        min_timestamp = input_data['Timestamp'].min()\n",
    "        max_timestamp = input_data['Timestamp'].max()\n",
    "        input_data['Timestamp'] = (input_data['Timestamp'] - min_timestamp) / (max_timestamp - min_timestamp)\n",
    "    \n",
    "    missing_indices = input_data['Determined Case ID'].isna()\n",
    "    missing_data = input_data[missing_indices].drop(columns=['Original Case ID', 'Determined Case ID'])\n",
    "    \n",
    "    if missing_indices.any():\n",
    "        target_values = complete_log['Case ID']\n",
    "        source_values = input_data.drop(columns=['Original Case ID', 'Determined Case ID'])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(source_values, target_values, test_size=0.2)\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_values = model.predict(missing_data)\n",
    "        elusive_log.loc[missing_indices, 'Determined Case ID'] = predicted_values\n",
    "        \n",
    "        print(\"Filled missing Case ID values using logistic regression.\")\n",
    "    \n",
    "    return elusive_log\n"
   ],
   "id": "f15166bf59e288b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:56.699213Z",
     "start_time": "2024-05-07T12:18:56.683562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_consistency_metrics(metrics: pd.DataFrame, configuration: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the consistency metrics DataFrame.\n",
    "    \n",
    "    :param metrics: DataFrame containing the consistency metrics.\n",
    "    :param configuration: The configuration dictionary.\n",
    "    :return: DataFrame containing the consistency metrics with the appropriate columns.\n",
    "    \"\"\"\n",
    "    possible_expert_attributes = ['Start Activity', 'End Activity', 'Directly Following']\n",
    "    included_attributes = [attr for attr in possible_expert_attributes if \n",
    "                           attr in configuration['expert_input_attributes']]\n",
    "    missing_attributes = [attr for attr in possible_expert_attributes if \n",
    "                          attr not in configuration['expert_input_attributes']]\n",
    "    \n",
    "    if included_attributes:\n",
    "        if ('Directly Following' in included_attributes and \n",
    "                'always' not in configuration['expert_input_values']['Directly Following']['occurrences']):\n",
    "            metrics.drop(columns=\"Di. Fo. Consistency\", inplace=True)\n",
    "    \n",
    "    if missing_attributes:\n",
    "        if 'Start Activity' in missing_attributes:\n",
    "            metrics.drop(columns=\"St. Ac. Consistency\", inplace=True)\n",
    "        if 'End Activity' in missing_attributes:\n",
    "            metrics.drop(columns=\"End Ac. Consistency\", inplace=True)\n",
    "        if 'Directly Following' in missing_attributes:\n",
    "            metrics.drop(columns=\"Di. Fo. Consistency\", inplace=True)\n",
    "    \n",
    "    return metrics\n"
   ],
   "id": "542a104565063188",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:57.106109Z",
     "start_time": "2024-05-07T12:18:57.082710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_metrics(metrics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the quality metrics DataFrame.\n",
    "\n",
    "    :param metrics: DataFrame containing the quality metrics.\n",
    "    :return: Formatted DataFrame.\n",
    "    \"\"\"\n",
    "    if metrics.empty:\n",
    "        return metrics\n",
    "\n",
    "    cols_to_format = metrics.columns.drop('Iteration')\n",
    "    metrics[cols_to_format] = metrics[cols_to_format].applymap(lambda x: '{:.2f}%'.format(x) if pd.notnull(x) else \"\")\n",
    "    metrics['Iteration'] = metrics['Iteration'].astype(int)\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "d8d8f1a9cedfcf2d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:57.605327Z",
     "start_time": "2024-05-07T12:18:57.589704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_input() -> Tuple[pd.DataFrame, str, str, dict, Tokenizer]:\n",
    "    \"\"\"\n",
    "    Prompt the user to input the path to the folder containing the repaired logs as well as the complete log with \n",
    "    corresponding name and the tokenizer for the case IDs, and retrieve the model configuration if available.\n",
    "\n",
    "    :return: A tuple containing the complete log DataFrame, the name of the complete log file, the path to the folder \n",
    "     containing the repaired logs after each iteration, and a dictionary representing the model configuration if \n",
    "     available, otherwise an empty dictionary, and a Tokenizer object representing the tokenizer if available,\n",
    "     otherwise a Tokenizer object initialized with None.\n",
    "    \"\"\"\n",
    "    complete_log_path = get_file_path(\"preprocessed complete log\")\n",
    "\n",
    "    if os.path.exists(complete_log_path):\n",
    "        complete_log = pd.read_csv(complete_log_path)\n",
    "        print(\"CSV file successfully read.\")\n",
    "        log_name = extract_log_name(complete_log_path)\n",
    "\n",
    "        if \"DISPLAY\" in os.environ:\n",
    "            root = tk.Tk()\n",
    "            root.withdraw()\n",
    "\n",
    "            folder_path = filedialog.askdirectory(\n",
    "                title=\"Select the folder that contains the repaired logs after each iteration\")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "        else:\n",
    "            folder_path = input(\"Enter the path to the folder that contains the repaired logs after each iteration: \")\n",
    "\n",
    "            if not folder_path:\n",
    "                raise ValueError(\"Error: No file selected.\")\n",
    "\n",
    "            folder_path = folder_path.strip('\"')\n",
    "\n",
    "        print(\"Folder path successfully read.\")\n",
    "        \n",
    "        configuration_path = get_file_path(\"model configuration\")\n",
    "        \n",
    "        if os.path.exists(configuration_path):\n",
    "            with open(configuration_path, 'rb') as file:\n",
    "                model_configuration = pickle.load(file)\n",
    "                print(\"Model configuration file successfully read.\")\n",
    "            \n",
    "            model_configuration = {key: value.to_dict() if isinstance(value, pd.DataFrame) else value for key, value in\n",
    "                                   model_configuration.items()}\n",
    "            \n",
    "            tokenizer_path = get_file_path(\"case ID tokenizer\")\n",
    "            \n",
    "            if os.path.exists(tokenizer_path):\n",
    "                tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "                print(\"Tokenizer successfully read.\")\n",
    "                \n",
    "                return complete_log, log_name, folder_path, model_configuration, tokenizer\n",
    "\n",
    "            return complete_log, log_name, folder_path, model_configuration, Tokenizer(None)\n",
    "        \n",
    "        return complete_log, log_name, folder_path, {}, Tokenizer(None)\n",
    "\n",
    "    return pd.DataFrame(), \"\", \"\", {}, Tokenizer(None)\n"
   ],
   "id": "f635854ae114b1f4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:57.990483Z",
     "start_time": "2024-05-07T12:18:57.974843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def perform_evaluation() -> None:\n",
    "    \"\"\"\n",
    "    Perform the evaluation of the repaired logs.\n",
    "    \"\"\"\n",
    "    complete_log, log_name, folder_path, model_configuration, tokenizer = get_input()\n",
    "    \n",
    "    if folder_path:\n",
    "        metrics, elusive_log = evaluate_repaired_logs(folder_path, log_name, complete_log, model_configuration)\n",
    "        random_log = randomize_missing_case_ids(elusive_log, tokenizer, log_name)\n",
    "        random_metrics = evaluate_other_log(metrics, random_log, complete_log, model_configuration)\n",
    "        random_metrics = format_consistency_metrics(random_metrics, model_configuration)\n",
    "        logistic_log = fill_missing_case_ids_logreg(complete_log, elusive_log, log_name)\n",
    "        logistic_metrics = evaluate_other_log(metrics, logistic_log, complete_log, model_configuration)\n",
    "        logistic_metrics = format_consistency_metrics(logistic_metrics, model_configuration)\n",
    "        process_metrics(metrics, log_name)\n",
    "        process_metrics(random_metrics, log_name, 'randomized', random_log)\n",
    "        process_metrics(logistic_metrics, log_name, 'logistic regression', logistic_log)\n"
   ],
   "id": "e9ec5cb5f083a59b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:58.449781Z",
     "start_time": "2024-05-07T12:18:58.419700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_metrics(metrics: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print the evaluation metrics in a tabular format.\n",
    "    \n",
    "    :param metrics: DataFrame containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    num_columns = len(metrics.columns) - 1\n",
    "    num_tables = (num_columns + 3) // 4\n",
    "    \n",
    "    for i in range(num_tables):\n",
    "        start_idx = 1 + 4 * i\n",
    "        end_idx = min(start_idx + 4, num_columns + 1)\n",
    "        indices = [0] + list(range(start_idx, end_idx))\n",
    "        metrics_table = metrics.iloc[:, indices]\n",
    "        print(tabulate(metrics_table, headers='keys', tablefmt='grid', showindex=False))\n"
   ],
   "id": "57f4364067a174a4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:58.904517Z",
     "start_time": "2024-05-07T12:18:58.873224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_metrics(metrics: pd.DataFrame, log_name: str, model_name: str = 'transformer', \n",
    "                    log: pd.DataFrame = pd. DataFrame()) -> None:\n",
    "    \"\"\"\n",
    "    Process the evaluation metrics and save them to a CSV file.\n",
    "    \n",
    "    :param metrics: DataFrame containing the evaluation metrics.\n",
    "    :param log_name: Name of the log file.\n",
    "    :param model_name: Name of the model used for evaluation. Default is 'transformer'.\n",
    "    :param log: DataFrame containing the log. Default is an empty DataFrame.\n",
    "    \"\"\"\n",
    "    metrics = add_elusive_equivalents(metrics)\n",
    "    metrics = format_metrics(metrics)\n",
    "    save_metrics(metrics, log_name, model_name, log)\n",
    "    print(\"\\n=== \" + model_name.upper() + \" ===\\n\")\n",
    "    print_metrics(metrics)\n"
   ],
   "id": "8b640d1990ea4e97",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:18:59.317868Z",
     "start_time": "2024-05-07T12:18:59.302246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def randomize_missing_case_ids(elusive_log: pd.DataFrame, tokenizer: Tokenizer, log_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomize the missing Case ID values in the elusive log.\n",
    "    \n",
    "    :param elusive_log: Log containing missing Case ID values.\n",
    "    :param tokenizer: Tokenizer object used to generate random case IDs.\n",
    "    :param log_name: Name of the log file.\n",
    "    :return: Log with missing Case ID values randomized.\n",
    "    \"\"\"\n",
    "    randomized_file = f\"evaluation/{log_name}/randomized/randomized_{log_name}.csv\"\n",
    "    \n",
    "    if os.path.exists(randomized_file) and os.path.getsize(randomized_file) > 0:\n",
    "        log = pd.read_csv(randomized_file)\n",
    "        print(\"Randomized log successfully read.\")\n",
    "        return log\n",
    "    \n",
    "    elusive_log['Determined Case ID'] = elusive_log['Original Case ID']\n",
    "    missing_indices = elusive_log['Determined Case ID'].isna()\n",
    "    \n",
    "    if missing_indices.any():\n",
    "        special_tokens = {\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\", \"[NONE]\"}\n",
    "        all_tokens = set(tokenizer.get_vocab().keys())\n",
    "        eligible_tokens = list(all_tokens - special_tokens)\n",
    "        \n",
    "        if not eligible_tokens:\n",
    "            raise ValueError(\"Tokenizer vocabulary does not contain any eligible tokens.\")\n",
    "        \n",
    "        num_missing = missing_indices.sum()\n",
    "        random_tokens = np.random.choice(eligible_tokens, size=num_missing, replace=True)\n",
    "        \n",
    "        if num_missing != len(random_tokens):\n",
    "            raise ValueError(\"Could not generate enough unique random tokens.\")\n",
    "        \n",
    "        elusive_log.loc[missing_indices, 'Determined Case ID'] = random_tokens\n",
    "        \n",
    "        print(\"Randomized missing Case ID values.\")\n",
    "        \n",
    "    return elusive_log\n"
   ],
   "id": "14840cf9ee8f2eb8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:19:00.085647Z",
     "start_time": "2024-05-07T12:19:00.070094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_metrics(metrics: pd.DataFrame, log_name: str, model_name: str, log: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Save evaluation metrics to a CSV file and save the log to a CSV file if applicable.\n",
    "    \n",
    "    :param metrics: DataFrame containing evaluation metrics.\n",
    "    :param log_name: Name of the log file.\n",
    "    :param model_name: Name of the model used for evaluation.\n",
    "    :param log: DataFrame containing the log.\n",
    "    \"\"\"\n",
    "    model_name = model_name.replace(' ', '_')\n",
    "    folder_name = f\"evaluation/{log_name}/{model_name}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    current_time = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    file_name = f'metrics_{current_time}.csv'\n",
    "    metrics.to_csv(os.path.join(folder_name, file_name), index=False)\n",
    "    \n",
    "    if model_name == 'randomized':\n",
    "        file_name = f'randomized_{log_name}.csv'\n",
    "        log.to_csv(os.path.join(folder_name, file_name), index=False)\n",
    "    elif model_name == 'logistic_regression':\n",
    "        file_name = f'logistic_regression_{log_name}.csv'\n",
    "        log.to_csv(os.path.join(folder_name, file_name), index=False)\n"
   ],
   "id": "2b88746e93f1974d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T12:19:18.105654Z",
     "start_time": "2024-05-07T12:19:01.447127Z"
    }
   },
   "cell_type": "code",
   "source": "perform_evaluation()\n",
   "id": "8364b4c7b44adda0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully read.\n",
      "Folder path successfully read.\n",
      "Model configuration file successfully read.\n",
      "Tokenizer successfully read.\n",
      "Randomized log successfully read.\n",
      "Logistic regression log successfully read.\n",
      "\n",
      "=== TRANSFORMER ===\n",
      "\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         | 0.00%                | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 92.86%         | 62.50%               | 83.33%     | 12.50%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           2 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           3 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           4 | 97.62%         | 87.50%               | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 83.33%             | 12.50%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           2 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           3 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           4 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|   Iteration | Real Case Accuracy   | Factual Case Accuracy   | Overall Case Accuracy   | St. Ac. Consistency   |\n",
      "+=============+======================+=========================+=========================+=======================+\n",
      "|           0 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           1 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           2 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           3 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           4 | 0.00%                | 0.00%                   | 0.00%                   | 100.00%               |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "+-------------+-----------------------+\n",
      "|   Iteration | End Ac. Consistency   |\n",
      "+=============+=======================+\n",
      "|           0 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           1 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           2 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           3 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           4 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "\n",
      "=== RANDOMIZED ===\n",
      "\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         | 0.00%                | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 100.00%        | 100.00%              | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|   Iteration | Real Case Accuracy   | Factual Case Accuracy   | Overall Case Accuracy   | St. Ac. Consistency   |\n",
      "+=============+======================+=========================+=========================+=======================+\n",
      "|           0 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           1 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "+-------------+-----------------------+\n",
      "|   Iteration | End Ac. Consistency   |\n",
      "+=============+=======================+\n",
      "|           0 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           1 | 100.00%               |\n",
      "+-------------+-----------------------+\n",
      "\n",
      "=== LOGISTIC REGRESSION ===\n",
      "\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|   Iteration | Completeness   | Elus. Completeness   | Accuracy   | Elus. Accuracy   |\n",
      "+=============+================+======================+============+==================+\n",
      "|           0 | 80.95%         | 0.00%                | 80.95%     | 0.00%            |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "|           1 | 100.00%        | 100.00%              | 85.71%     | 25.00%           |\n",
      "+-------------+----------------+----------------------+------------+------------------+\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|   Iteration | Factual Accuracy   | Elus. Factual Accuracy   | Overall Accuracy   | Elus. Overall Accuracy   |\n",
      "+=============+====================+==========================+====================+==========================+\n",
      "|           0 | 0.00%              | 0.00%                    | 80.95%             | 0.00%                    |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "|           1 | 0.00%              | 0.00%                    | 85.71%             | 25.00%                   |\n",
      "+-------------+--------------------+--------------------------+--------------------+--------------------------+\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|   Iteration | Real Case Accuracy   | Factual Case Accuracy   | Overall Case Accuracy   | St. Ac. Consistency   |\n",
      "+=============+======================+=========================+=========================+=======================+\n",
      "|           0 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "|           1 | 0.00%                | 0.00%                   | 0.00%                   | 83.33%                |\n",
      "+-------------+----------------------+-------------------------+-------------------------+-----------------------+\n",
      "+-------------+-----------------------+\n",
      "|   Iteration | End Ac. Consistency   |\n",
      "+=============+=======================+\n",
      "|           0 | 83.33%                |\n",
      "+-------------+-----------------------+\n",
      "|           1 | 100.00%               |\n",
      "+-------------+-----------------------+\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
